diff --git a/configure b/configure
index 34c2adb4a4..9325cc8bca 100755
--- a/configure
+++ b/configure
@@ -254,6 +254,7 @@ External library support:
   --enable-libopenmpt      enable decoding tracked files via libopenmpt [no]
   --enable-libopus         enable Opus de/encoding via libopus [no]
   --enable-libpulse        enable Pulseaudio input via libpulse [no]
+  --enable-librav1e        enable AV1 encoding via rav1e [no]
   --enable-librsvg         enable SVG rasterization via librsvg [no]
   --enable-librubberband   enable rubberband needed for rubberband filter [no]
   --enable-librtmp         enable RTMP[E] support via librtmp [no]
@@ -264,6 +265,7 @@ External library support:
   --enable-libspeex        enable Speex de/encoding via libspeex [no]
   --enable-libsrt          enable Haivision SRT protocol via libsrt [no]
   --enable-libssh          enable SFTP protocol via libssh [no]
+  --enable-libsvtav1       enable AV1 encoding via svt [no]
   --enable-libtensorflow   enable TensorFlow as a DNN module backend
                            for DNN based filters like sr [no]
   --enable-libtesseract    enable Tesseract, needed for ocr filter [no]
@@ -1784,6 +1786,7 @@ EXTERNAL_LIBRARY_LIST="
     libopenmpt
     libopus
     libpulse
+    librav1e
     librsvg
     librtmp
     libshine
@@ -1793,6 +1796,7 @@ EXTERNAL_LIBRARY_LIST="
     libspeex
     libsrt
     libssh
+    libsvtav1
     libtensorflow
     libtesseract
     libtheora
@@ -3185,12 +3189,14 @@ libopenmpt_demuxer_deps="libopenmpt"
 libopus_decoder_deps="libopus"
 libopus_encoder_deps="libopus"
 libopus_encoder_select="audio_frame_queue"
+librav1e_encoder_deps="librav1e"
 librsvg_decoder_deps="librsvg"
 libshine_encoder_deps="libshine"
 libshine_encoder_select="audio_frame_queue"
 libspeex_decoder_deps="libspeex"
 libspeex_encoder_deps="libspeex"
 libspeex_encoder_select="audio_frame_queue"
+libsvt_av1_encoder_deps="libsvtav1"
 libtheora_encoder_deps="libtheora"
 libtwolame_encoder_deps="libtwolame"
 libvo_amrwbenc_encoder_deps="libvo_amrwbenc"
@@ -3487,6 +3493,10 @@ nlmeans_opencl_filter_deps="opencl"
 nnedi_filter_deps="gpl"
 ocr_filter_deps="libtesseract"
 ocv_filter_deps="libopencv"
+perceptual_filter_deps="libopencv"
+perceptual_filter_extralibs="-lstdc++ -lopencv_img_hash"
+img_hash_filter_deps="libopencv"
+img_hash_filter_extralibs="-lstdc++ -lopencv_img_hash"
 openclsrc_filter_deps="opencl"
 overlay_opencl_filter_deps="opencl"
 overlay_qsv_filter_deps="libmfx"
@@ -6252,6 +6262,7 @@ enabled libopus           && {
     }
 }
 enabled libpulse          && require_pkg_config libpulse libpulse pulse/pulseaudio.h pa_context_new
+enabled librav1e          && require_pkg_config librav1e rav1e rav1e.h rav1e_context_new
 enabled librsvg           && require_pkg_config librsvg librsvg-2.0 librsvg-2.0/librsvg/rsvg.h rsvg_handle_render_cairo
 enabled librtmp           && require_pkg_config librtmp librtmp librtmp/rtmp.h RTMP_Socket
 enabled librubberband     && require_pkg_config librubberband "rubberband >= 1.8.1" rubberband/rubberband-c.h rubberband_new -lstdc++ && append librubberband_extralibs "-lstdc++"
@@ -6263,6 +6274,7 @@ enabled libsoxr           && require libsoxr soxr.h soxr_create -lsoxr
 enabled libssh            && require_pkg_config libssh libssh libssh/sftp.h sftp_init
 enabled libspeex          && require_pkg_config libspeex speex speex/speex.h speex_decoder_init
 enabled libsrt            && require_pkg_config libsrt "srt >= 1.3.0" srt/srt.h srt_socket
+enabled libsvtav1         && require_pkg_config libsvtav1 SvtAv1Enc EbSvtAv1Enc.h eb_init_handle
 enabled libtensorflow     && require libtensorflow tensorflow/c/c_api.h TF_Version -ltensorflow
 enabled libtesseract      && require_pkg_config libtesseract tesseract tesseract/capi.h TessBaseAPICreate
 enabled libtheora         && require libtheora theora/theoraenc.h th_info_init -ltheoraenc -ltheoradec -logg
diff --git a/doc/encoders.texi b/doc/encoders.texi
index eefd124751..2ed7053838 100644
--- a/doc/encoders.texi
+++ b/doc/encoders.texi
@@ -1378,6 +1378,35 @@ makes it possible to store non-rgb pix_fmts.
 
 @end table
 
+@section librav1e
+
+rav1e AV1 encoder wrapper.
+
+Requires the presence of the rav1e headers and library from crav1e
+during configuration. You need to explicitly configue the build with
+@code{--enable-librav1e}.
+
+@subsection Options
+
+@table @option
+@item max-quantizer
+Sets the maximum qauntizer (floor) to use when using bitrate mode.
+
+@item quantizer
+Uses quantizers mode to encode at the given quantizer.
+
+@item rav1e-params
+Set rav1e options using a list of @var{key}=@var{value} couples separated
+by ":". See @command{rav1e --help} for a list of options.
+
+For example to specify librav1e encoding options with @option{-rav1e-params}:
+
+@example
+ffmpeg -i input -c:v librav1e -rav1e-params speed=5:low_latency=true output.mp4
+@end example
+
+@end table
+
 @section libaom-av1
 
 libaom AV1 encoder wrapper.
diff --git a/doc/general.texi b/doc/general.texi
index 3c0c803449..50939f7315 100644
--- a/doc/general.texi
+++ b/doc/general.texi
@@ -243,6 +243,13 @@ FFmpeg can use the OpenJPEG libraries for decoding/encoding J2K videos.  Go to
 instructions.  To enable using OpenJPEG in FFmpeg, pass @code{--enable-libopenjpeg} to
 @file{./configure}.
 
+@section rav1e
+
+FFmpeg can make use of rav1e (Rust AV1 Encoder) via its C bindings to encode videos.
+Go to @url{https://github.com/lu-zero/crav1e/} and @url{https://github.com/xiph/rav1e/}
+and follow the instructions. To enable using rav1e in FFmpeg, pass @code{--enable-librav1e}
+to @file{./configure}.
+
 @section TwoLAME
 
 FFmpeg can make use of the TwoLAME library for MP2 encoding.
diff --git a/libavcodec/Makefile b/libavcodec/Makefile
index 3cd73fbcc6..65c9966bac 100644
--- a/libavcodec/Makefile
+++ b/libavcodec/Makefile
@@ -988,9 +988,11 @@ OBJS-$(CONFIG_LIBOPUS_DECODER)            += libopusdec.o libopus.o     \
                                              vorbis_data.o
 OBJS-$(CONFIG_LIBOPUS_ENCODER)            += libopusenc.o libopus.o     \
                                              vorbis_data.o
+OBJS-$(CONFIG_LIBRAV1E_ENCODER)           += librav1e.o
 OBJS-$(CONFIG_LIBSHINE_ENCODER)           += libshine.o
 OBJS-$(CONFIG_LIBSPEEX_DECODER)           += libspeexdec.o
 OBJS-$(CONFIG_LIBSPEEX_ENCODER)           += libspeexenc.o
+OBJS-$(CONFIG_LIBSVT_AV1_ENCODER)         += libsvt_av1.o
 OBJS-$(CONFIG_LIBTHEORA_ENCODER)          += libtheoraenc.o
 OBJS-$(CONFIG_LIBTWOLAME_ENCODER)         += libtwolame.o
 OBJS-$(CONFIG_LIBVO_AMRWBENC_ENCODER)     += libvo-amrwbenc.o
diff --git a/libavcodec/allcodecs.c b/libavcodec/allcodecs.c
index d2f9a39ce5..13ef88ea2a 100644
--- a/libavcodec/allcodecs.c
+++ b/libavcodec/allcodecs.c
@@ -703,10 +703,12 @@ extern AVCodec ff_libopenjpeg_encoder;
 extern AVCodec ff_libopenjpeg_decoder;
 extern AVCodec ff_libopus_encoder;
 extern AVCodec ff_libopus_decoder;
+extern AVCodec ff_librav1e_encoder;
 extern AVCodec ff_librsvg_decoder;
 extern AVCodec ff_libshine_encoder;
 extern AVCodec ff_libspeex_encoder;
 extern AVCodec ff_libspeex_decoder;
+extern AVCodec ff_libsvt_av1_encoder;
 extern AVCodec ff_libtheora_encoder;
 extern AVCodec ff_libtwolame_encoder;
 extern AVCodec ff_libvo_amrwbenc_encoder;
diff --git a/libavcodec/librav1e.c b/libavcodec/librav1e.c
new file mode 100644
index 0000000000..cdd12a2e3c
--- /dev/null
+++ b/libavcodec/librav1e.c
@@ -0,0 +1,596 @@
+/*
+ * librav1e encoder
+ *
+ * Copyright (c) 2019 Derek Buitenhuis
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <rav1e.h>
+
+#include "libavutil/internal.h"
+#include "libavutil/avassert.h"
+#include "libavutil/base64.h"
+#include "libavutil/common.h"
+#include "libavutil/mathematics.h"
+#include "libavutil/opt.h"
+#include "libavutil/pixdesc.h"
+#include "avcodec.h"
+#include "internal.h"
+
+typedef struct librav1eContext {
+    const AVClass *class;
+
+    RaContext *ctx;
+    AVBSFContext *bsf;
+
+    uint8_t *pass_data;
+    size_t pass_pos;
+    int pass_size;
+
+    char *rav1e_opts;
+    int quantizer;
+    int speed;
+    int tiles;
+    int tile_rows;
+    int tile_cols;
+} librav1eContext;
+
+static inline RaPixelRange range_map(enum AVPixelFormat pix_fmt, enum AVColorRange range)
+{
+    switch (pix_fmt) {
+    case AV_PIX_FMT_YUVJ420P:
+    case AV_PIX_FMT_YUVJ422P:
+    case AV_PIX_FMT_YUVJ444P:
+        return RA_PIXEL_RANGE_FULL;
+    }
+
+    switch (range) {
+    case AVCOL_RANGE_JPEG:
+        return RA_PIXEL_RANGE_FULL;
+    case AVCOL_RANGE_MPEG:
+    default:
+        return RA_PIXEL_RANGE_LIMITED;
+    }
+}
+
+static inline RaChromaSampling pix_fmt_map(enum AVPixelFormat pix_fmt)
+{
+    switch (pix_fmt) {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUVJ420P:
+    case AV_PIX_FMT_YUV420P10:
+    case AV_PIX_FMT_YUV420P12:
+        return RA_CHROMA_SAMPLING_CS420;
+    case AV_PIX_FMT_YUV422P:
+    case AV_PIX_FMT_YUVJ422P:
+    case AV_PIX_FMT_YUV422P10:
+    case AV_PIX_FMT_YUV422P12:
+        return RA_CHROMA_SAMPLING_CS422;
+    case AV_PIX_FMT_YUV444P:
+    case AV_PIX_FMT_YUVJ444P:
+    case AV_PIX_FMT_YUV444P10:
+    case AV_PIX_FMT_YUV444P12:
+        return RA_CHROMA_SAMPLING_CS444;
+    default:
+        av_assert0(0);
+    }
+}
+
+static inline RaChromaSamplePosition chroma_loc_map(enum AVChromaLocation chroma_loc)
+{
+    switch (chroma_loc) {
+    case AVCHROMA_LOC_LEFT:
+        return RA_CHROMA_SAMPLE_POSITION_VERTICAL;
+    case AVCHROMA_LOC_TOPLEFT:
+        return RA_CHROMA_SAMPLE_POSITION_COLOCATED;
+    default:
+        return RA_CHROMA_SAMPLE_POSITION_UNKNOWN;
+    }
+}
+
+static int get_stats(AVCodecContext *avctx, int eos)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    RaData* buf = rav1e_twopass_out(ctx->ctx);
+    if (!buf)
+        return 0;
+
+    if (!eos) {
+        uint8_t *tmp = av_fast_realloc(ctx->pass_data, &ctx->pass_size,
+                                      ctx->pass_pos + buf->len);
+        if (!tmp) {
+            rav1e_data_unref(buf);
+            return AVERROR(ENOMEM);
+        }
+
+        ctx->pass_data = tmp;
+        memcpy(ctx->pass_data + ctx->pass_pos, buf->data, buf->len);
+        ctx->pass_pos += buf->len;
+    } else {
+        size_t b64_size = AV_BASE64_SIZE(ctx->pass_pos);
+
+        memcpy(ctx->pass_data, buf->data, buf->len);
+
+        avctx->stats_out = av_malloc(b64_size);
+        if (!avctx->stats_out) {
+            rav1e_data_unref(buf);
+            return AVERROR(ENOMEM);
+        }
+
+        av_base64_encode(avctx->stats_out, b64_size, ctx->pass_data, ctx->pass_pos);
+
+        av_freep(&ctx->pass_data);
+    }
+
+    rav1e_data_unref(buf);
+
+    return 0;
+}
+
+static int set_stats(AVCodecContext *avctx)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    int ret = 1;
+
+    while (ret > 0 && ctx->pass_size - ctx->pass_pos > 0) {
+        ret = rav1e_twopass_in(ctx->ctx, ctx->pass_data + ctx->pass_pos, ctx->pass_size);
+        if (ret < 0)
+            return AVERROR_EXTERNAL;
+        ctx->pass_pos += ret;
+    }
+
+    return 0;
+}
+
+static av_cold int librav1e_encode_close(AVCodecContext *avctx)
+{
+    librav1eContext *ctx = avctx->priv_data;
+
+    if (ctx->ctx) {
+        rav1e_context_unref(ctx->ctx);
+        ctx->ctx = NULL;
+    }
+
+    av_bsf_free(&ctx->bsf);
+    av_freep(&ctx->pass_data);
+
+    return 0;
+}
+
+static av_cold int librav1e_encode_init(AVCodecContext *avctx)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(avctx->pix_fmt);
+    RaConfig *cfg = NULL;
+    int rret;
+    int ret = 0;
+
+    cfg = rav1e_config_default();
+    if (!cfg) {
+        av_log(avctx, AV_LOG_ERROR, "Could not allocate rav1e config.\n");
+        return AVERROR_EXTERNAL;
+    }
+
+    rav1e_config_set_time_base(cfg, (RaRational) {
+                               avctx->time_base.num * avctx->ticks_per_frame,
+                               avctx->time_base.den
+                               });
+
+    if (avctx->flags & AV_CODEC_FLAG_PASS2) {
+        if (!avctx->stats_in) {
+            av_log(avctx, AV_LOG_ERROR, "No stats file provided for second pass.\n");
+            ret = AVERROR(EINVAL);
+            goto end;
+        }
+
+        ctx->pass_size = (strlen(avctx->stats_in) * 3) / 4;
+        ctx->pass_data = av_malloc(ctx->pass_size);
+        if (!ctx->pass_data) {
+            av_log(avctx, AV_LOG_ERROR, "Could not allocate stats buffer.\n");
+            ret = AVERROR(ENOMEM);
+            goto end;
+        }
+
+        ctx->pass_size = av_base64_decode(ctx->pass_data, avctx->stats_in, ctx->pass_size);
+        if (ctx->pass_size < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Invalid pass file.\n");
+            ret = AVERROR(EINVAL);
+            goto end;
+        }
+    }
+
+    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {
+         const AVBitStreamFilter *filter = av_bsf_get_by_name("extract_extradata");
+         int bret;
+
+         if (!filter) {
+            av_log(avctx, AV_LOG_ERROR, "extract_extradata bitstream filter "
+                   "not found. This is a bug, please report it.\n");
+            ret = AVERROR_BUG;
+            goto end;
+         }
+
+         bret = av_bsf_alloc(filter, &ctx->bsf);
+         if (bret < 0) {
+             ret = bret;
+             goto end;
+         }
+
+         bret = avcodec_parameters_from_context(ctx->bsf->par_in, avctx);
+         if (bret < 0) {
+             ret = bret;
+             goto end;
+         }
+
+         bret = av_bsf_init(ctx->bsf);
+         if (bret < 0) {
+             ret = bret;
+             goto end;
+         }
+    }
+
+    if (ctx->rav1e_opts) {
+        AVDictionary *dict    = NULL;
+        AVDictionaryEntry *en = NULL;
+
+        if (!av_dict_parse_string(&dict, ctx->rav1e_opts, "=", ":", 0)) {
+            while (en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)) {
+                int parse_ret = rav1e_config_parse(cfg, en->key, en->value);
+                if (parse_ret < 0)
+                    av_log(avctx, AV_LOG_WARNING, "Invalid value for %s: %s.\n", en->key, en->value);
+            }
+            av_dict_free(&dict);
+        }
+    }
+
+    rret = rav1e_config_parse_int(cfg, "width", avctx->width);
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Invalid width passed to rav1e.\n");
+        ret = AVERROR_INVALIDDATA;
+        goto end;
+    }
+
+    rret = rav1e_config_parse_int(cfg, "height", avctx->height);
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Invalid height passed to rav1e.\n");
+        ret = AVERROR_INVALIDDATA;
+        goto end;
+    }
+
+    rret = rav1e_config_parse_int(cfg, "threads", avctx->thread_count);
+    if (rret < 0)
+        av_log(avctx, AV_LOG_WARNING, "Invalid number of threads, defaulting to auto.\n");
+
+    if (ctx->speed >= 0) {
+        rret = rav1e_config_parse_int(cfg, "speed", ctx->speed);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set speed preset.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+
+    /* rav1e handles precedence between 'tiles' and cols/rows for us. */
+    if (ctx->tiles >= 0) {
+        rret = rav1e_config_parse_int(cfg, "tiles", ctx->tiles);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set number of tiles to encode with.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+    if (ctx->tile_rows >= 0) {
+        rret = rav1e_config_parse_int(cfg, "tile_rows", ctx->tile_rows);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set number of tile rows to encode with.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+    if (ctx->tile_cols >= 0) {
+        rret = rav1e_config_parse_int(cfg, "tile_cols_log2", ctx->tile_cols);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set number of tile cols to encode with.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+
+    if (avctx->gop_size > 0) {
+        rret = rav1e_config_parse_int(cfg, "key_frame_interval", avctx->gop_size);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set max keyint.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+
+    if (avctx->keyint_min > 0) {
+        rret = rav1e_config_parse_int(cfg, "min_key_frame_interval", avctx->keyint_min);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set min keyint.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+
+    if (avctx->bit_rate && ctx->quantizer < 0) {
+        int max_quantizer = avctx->qmax >= 0 ? avctx->qmax : 255;
+
+        rret = rav1e_config_parse_int(cfg, "quantizer", max_quantizer);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set max quantizer.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+
+        if (avctx->qmin >= 0) {
+            rret = rav1e_config_parse_int(cfg, "min_quantizer", avctx->qmin);
+            if (rret < 0) {
+                av_log(avctx, AV_LOG_ERROR, "Could not set min quantizer.\n");
+                ret = AVERROR_EXTERNAL;
+                goto end;
+            }
+        }
+
+        rret = rav1e_config_parse_int(cfg, "bitrate", avctx->bit_rate);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set bitrate.\n");
+            ret = AVERROR_INVALIDDATA;
+            goto end;
+        }
+    } else if (ctx->quantizer >= 0) {
+        rret = rav1e_config_parse_int(cfg, "quantizer", ctx->quantizer);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set quantizer.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+
+    rret = rav1e_config_set_pixel_format(cfg, desc->comp[0].depth,
+                                         pix_fmt_map(avctx->pix_fmt),
+                                         chroma_loc_map(avctx->chroma_sample_location),
+                                         range_map(avctx->pix_fmt, avctx->color_range));
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Failed to set pixel format properties.\n");
+        ret = AVERROR_INVALIDDATA;
+        goto end;
+    }
+
+    /* rav1e's colorspace enums match standard values. */
+    rret = rav1e_config_set_color_description(cfg, (RaMatrixCoefficients) avctx->colorspace,
+                                              (RaColorPrimaries) avctx->color_primaries,
+                                              (RaTransferCharacteristics) avctx->color_trc);
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_WARNING, "Failed to set color properties.\n");
+        if (avctx->err_recognition & AV_EF_EXPLODE) {
+            ret = AVERROR_INVALIDDATA;
+            goto end;
+        }
+    }
+
+    ctx->ctx = rav1e_context_new(cfg);
+    if (!ctx->ctx) {
+        av_log(avctx, AV_LOG_ERROR, "Failed to create rav1e encode context.\n");
+        ret = AVERROR_EXTERNAL;
+        goto end;
+    }
+
+    ret = 0;
+
+end:
+
+    rav1e_config_unref(cfg);
+
+    return ret;
+}
+
+static int librav1e_send_frame(AVCodecContext *avctx, const AVFrame *frame)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    RaFrame *rframe = NULL;
+    int ret;
+
+    if (frame) {
+        const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(frame->format);
+
+        rframe = rav1e_frame_new(ctx->ctx);
+        if (!rframe) {
+            av_log(avctx, AV_LOG_ERROR, "Could not allocate new rav1e frame.\n");
+            return AVERROR(ENOMEM);
+        }
+
+        for (int i = 0; i < desc->nb_components; i++) {
+            int shift = i ? desc->log2_chroma_h : 0;
+            int bytes = desc->comp[0].depth == 8 ? 1 : 2;
+            rav1e_frame_fill_plane(rframe, i, frame->data[i],
+                                   (frame->height >> shift) * frame->linesize[i],
+                                   frame->linesize[i], bytes);
+        }
+    }
+
+    ret = rav1e_send_frame(ctx->ctx, rframe);
+    if (rframe)
+         rav1e_frame_unref(rframe); /* No need to unref if flushing. */
+
+    switch (ret) {
+    case RA_ENCODER_STATUS_SUCCESS:
+        break;
+    case RA_ENCODER_STATUS_ENOUGH_DATA:
+        return AVERROR(EAGAIN);
+    case RA_ENCODER_STATUS_FAILURE:
+        av_log(avctx, AV_LOG_ERROR, "Could not send frame.\n");
+        return AVERROR_EXTERNAL;
+    default:
+        av_log(avctx, AV_LOG_ERROR, "Unknown return code %d from rav1e_send_frame.\n", ret);
+        return AVERROR_UNKNOWN;
+    }
+
+    return 0;
+}
+
+static void librav1e_packet_unref(void *opaque, uint8_t *data)
+{
+    RaPacket *rpkt = opaque;
+
+    rav1e_packet_unref(rpkt);
+}
+
+static int librav1e_receive_packet(AVCodecContext *avctx, AVPacket *pkt)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    RaPacket *rpkt = NULL;
+    int ret;
+
+retry:
+
+    if (avctx->flags & AV_CODEC_FLAG_PASS1) {
+        int sret = get_stats(avctx, 0);
+        if (sret < 0)
+            return sret;
+    } else if (avctx->flags & AV_CODEC_FLAG_PASS2) {
+        int sret = set_stats(avctx);
+        if (sret < 0)
+            return sret;
+    }
+
+    ret = rav1e_receive_packet(ctx->ctx, &rpkt);
+    switch (ret) {
+    case RA_ENCODER_STATUS_SUCCESS:
+        break;
+    case RA_ENCODER_STATUS_LIMIT_REACHED:
+        if (avctx->flags & AV_CODEC_FLAG_PASS1) {
+            int sret = get_stats(avctx, 1);
+            if (sret < 0)
+                return sret;
+        }
+        return AVERROR_EOF;
+    case RA_ENCODER_STATUS_ENCODED:
+        if (avctx->internal->draining)
+            goto retry;
+        return AVERROR(EAGAIN);
+    case RA_ENCODER_STATUS_NEED_MORE_DATA:
+        if (avctx->internal->draining) {
+            av_log(avctx, AV_LOG_ERROR, "Unexpected error when receiving packet after EOF.\n");
+            return AVERROR_EXTERNAL;
+        }
+        return AVERROR(EAGAIN);
+    case RA_ENCODER_STATUS_FAILURE:
+        av_log(avctx, AV_LOG_ERROR, "Could not encode frame.\n");
+        return AVERROR_EXTERNAL;
+    default:
+        av_log(avctx, AV_LOG_ERROR, "Unknown return code %d from rav1e_receive_packet.\n", ret);
+        return AVERROR_UNKNOWN;
+    }
+
+    pkt->buf = av_buffer_create((uint8_t *) rpkt->data, rpkt->len, librav1e_packet_unref,
+                                rpkt, AV_BUFFER_FLAG_READONLY);
+    if (!pkt->buf) {
+        rav1e_packet_unref(rpkt);
+        return AVERROR(ENOMEM);
+    }
+
+    pkt->data = (uint8_t *) rpkt->data;
+    pkt->size = rpkt->len;
+
+    if (rpkt->frame_type == RA_FRAME_TYPE_KEY)
+        pkt->flags |= AV_PKT_FLAG_KEY;
+
+    pkt->pts = pkt->dts = rpkt->input_frameno * avctx->ticks_per_frame;
+
+    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {
+        int ret = av_bsf_send_packet(ctx->bsf, pkt);
+        if (ret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "extradata extraction send failed.\n");
+            av_packet_unref(pkt);
+            return ret;
+        }
+
+        ret = av_bsf_receive_packet(ctx->bsf, pkt);
+        if (ret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "extradata extraction receive failed.\n");
+            av_packet_unref(pkt);
+            return ret;
+        }
+    }
+
+    return 0;
+}
+
+#define OFFSET(x) offsetof(librav1eContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+
+static const AVOption options[] = {
+    { "qp", "use constant quantizer mode", OFFSET(quantizer), AV_OPT_TYPE_INT, { .i64 = -1 }, -1, 255, VE },
+    { "speed", "what speed preset to use", OFFSET(speed), AV_OPT_TYPE_INT, { .i64 = -1 }, -1, 10, VE },
+    { "tiles", "number of tiles encode with", OFFSET(tiles), AV_OPT_TYPE_INT, { .i64 = -1 }, -1, INT64_MAX, VE },
+    { "tile-rows", "number of tiles rows to encode with", OFFSET(tile_rows), AV_OPT_TYPE_INT, { .i64 = -1 }, -1, INT64_MAX, VE },
+    { "tile-columns", "number of tiles columns to encode with", OFFSET(tile_cols), AV_OPT_TYPE_INT, { .i64 = -1 }, -1, INT64_MAX, VE },
+    { "rav1e-params", "set the rav1e configuration using a :-separated list of key=value parameters", OFFSET(rav1e_opts), AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+    { NULL }
+};
+
+static const AVCodecDefault librav1e_defaults[] = {
+    { "g",           "0" },
+    { "keyint_min",  "0" },
+    { "qmax",       "-1" },
+    { "qmin",       "-1" },
+    { NULL }
+};
+
+const enum AVPixelFormat librav1e_pix_fmts[] = {
+    AV_PIX_FMT_YUV420P,
+    AV_PIX_FMT_YUVJ420P,
+    AV_PIX_FMT_YUV420P10,
+    AV_PIX_FMT_YUV420P12,
+    AV_PIX_FMT_YUV422P,
+    AV_PIX_FMT_YUVJ422P,
+    AV_PIX_FMT_YUV422P10,
+    AV_PIX_FMT_YUV422P12,
+    AV_PIX_FMT_YUV444P,
+    AV_PIX_FMT_YUVJ444P,
+    AV_PIX_FMT_YUV444P10,
+    AV_PIX_FMT_YUV444P12,
+    AV_PIX_FMT_NONE
+};
+
+static const AVClass class = {
+    .class_name = "librav1e",
+    .item_name  = av_default_item_name,
+    .option     = options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+AVCodec ff_librav1e_encoder = {
+    .name           = "librav1e",
+    .long_name      = NULL_IF_CONFIG_SMALL("librav1e AV1"),
+    .type           = AVMEDIA_TYPE_VIDEO,
+    .id             = AV_CODEC_ID_AV1,
+    .init           = librav1e_encode_init,
+    .send_frame     = librav1e_send_frame,
+    .receive_packet = librav1e_receive_packet,
+    .close          = librav1e_encode_close,
+    .priv_data_size = sizeof(librav1eContext),
+    .priv_class     = &class,
+    .defaults       = librav1e_defaults,
+    .pix_fmts       = librav1e_pix_fmts,
+    .capabilities   = AV_CODEC_CAP_DELAY | AV_CODEC_CAP_AUTO_THREADS,
+    .caps_internal  = FF_CODEC_CAP_INIT_CLEANUP,
+    .wrapper_name   = "librav1e",
+};
diff --git a/libavcodec/libsvt_av1.c b/libavcodec/libsvt_av1.c
new file mode 100644
index 0000000000..1523542107
--- /dev/null
+++ b/libavcodec/libsvt_av1.c
@@ -0,0 +1,515 @@
+/*
+* Scalable Video Technology for AV1 encoder library plugin
+*
+* Copyright (c) 2018 Intel Corporation
+*
+* This file is part of FFmpeg.
+*
+* FFmpeg is free software; you can redistribute it and/or
+* modify it under the terms of the GNU Lesser General Public
+* License as published by the Free Software Foundation; either
+* version 2.1 of the License, or (at your option) any later version.
+*
+* FFmpeg is distributed in the hope that it will be useful,
+* but WITHOUT ANY WARRANTY; without even the implied warranty of
+* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+* Lesser General Public License for more details.
+*
+* You should have received a copy of the GNU Lesser General Public
+* License along with this program; if not, write to the Free Software
+* Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+*/
+
+#include <stdint.h>
+#include "EbSvtAv1ErrorCodes.h"
+#include "EbSvtAv1Enc.h"
+
+#include "libavutil/common.h"
+#include "libavutil/frame.h"
+#include "libavutil/opt.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/avassert.h"
+
+#include "internal.h"
+#include "avcodec.h"
+
+typedef enum eos_status {
+    EOS_NOT_REACHED = 0,
+    EOS_SENT,
+    EOS_RECEIVED
+}EOS_STATUS;
+
+typedef struct SvtContext {
+    AVClass     *class;
+
+    EbSvtAv1EncConfiguration    enc_params;
+    EbComponentType            *svt_handle;
+
+    EbBufferHeaderType         *in_buf;
+    int                         raw_size;
+
+    EOS_STATUS eos_flag;
+
+    // User options.
+    int hierarchical_level;
+    int la_depth;
+    int enc_mode;
+    int rc_mode;
+    int scd;
+    int qp;
+
+    int forced_idr;
+
+    int tier;
+    int level;
+    int profile;
+
+    int base_layer_switch_mode;
+} SvtContext;
+
+static const struct {
+    EbErrorType    eb_err;
+    int            av_err;
+    const char     *desc;
+} svt_errors[] = {
+    { EB_ErrorNone,                             0,              "success"                   },
+    { EB_ErrorInsufficientResources,      AVERROR(ENOMEM),      "insufficient resources"    },
+    { EB_ErrorUndefined,                  AVERROR(EINVAL),      "undefined error"           },
+    { EB_ErrorInvalidComponent,           AVERROR(EINVAL),      "invalid component"         },
+    { EB_ErrorBadParameter,               AVERROR(EINVAL),      "bad parameter"             },
+    { EB_ErrorDestroyThreadFailed,        AVERROR_EXTERNAL,     "failed to destory thread"  },
+    { EB_ErrorSemaphoreUnresponsive,      AVERROR_EXTERNAL,     "semaphore unresponsive"    },
+    { EB_ErrorDestroySemaphoreFailed,     AVERROR_EXTERNAL,     "semaphore unresponsive"    },
+    { EB_ErrorCreateMutexFailed,          AVERROR_EXTERNAL,     "failed to creat mutex"     },
+    { EB_ErrorMutexUnresponsive,          AVERROR_EXTERNAL,     "mutex unresponsive"        },
+    { EB_ErrorDestroyMutexFailed,         AVERROR_EXTERNAL,     "failed to destory muxtex"  },
+    { EB_NoErrorEmptyQueue,               AVERROR(EAGAIN),      "empty queue"               },
+};
+
+static int svt_map_error(EbErrorType eb_err, const char **desc)
+{
+    int i;
+
+    av_assert0(desc);
+    for (i = 0; i < FF_ARRAY_ELEMS(svt_errors); i++) {
+        if (svt_errors[i].eb_err == eb_err) {
+            *desc = svt_errors[i].desc;
+            return svt_errors[i].av_err;
+        }
+    }
+    *desc = "unknown error";
+    return AVERROR_UNKNOWN;
+}
+
+static int svt_print_error(void *log_ctx, EbErrorType err,
+                           const char *error_string)
+{
+    const char *desc;
+    int ret;
+    ret = svt_map_error(err, &desc);
+    av_log(log_ctx, AV_LOG_ERROR, "%s: %s (0x%x)\n", error_string, desc, err);
+    return ret;
+}
+
+static void free_buffer(SvtContext *svt_enc)
+{
+    if (svt_enc->in_buf) {
+        EbSvtIOFormat *in_data = (EbSvtIOFormat *)svt_enc->in_buf->p_buffer;
+        av_freep(&in_data);
+        av_freep(&svt_enc->in_buf);
+    }
+}
+
+static int alloc_buffer(EbSvtAv1EncConfiguration *config, SvtContext *svt_enc)
+{
+    const int    pack_mode_10bit   =
+        (config->encoder_bit_depth > 8) && (config->compressed_ten_bit_format == 0) ? 1 : 0;
+    const size_t luma_size_8bit    =
+        config->source_width * config->source_height * (1 << pack_mode_10bit);
+    const size_t luma_size_10bit   =
+        (config->encoder_bit_depth > 8 && pack_mode_10bit == 0) ? luma_size_8bit : 0;
+
+    EbSvtIOFormat *in_data;
+
+    svt_enc->raw_size = (luma_size_8bit + luma_size_10bit) * 3 / 2;
+
+    // allocate buffer for in and out
+    svt_enc->in_buf           = av_mallocz(sizeof(*svt_enc->in_buf));
+    if (!svt_enc->in_buf)
+        return AVERROR(ENOMEM);
+
+    in_data  = av_mallocz(sizeof(*in_data));
+    if (!in_data)
+        return AVERROR(ENOMEM);
+    svt_enc->in_buf->p_buffer  = (unsigned char *)in_data;
+
+    svt_enc->in_buf->size        = sizeof(*svt_enc->in_buf);
+    svt_enc->in_buf->p_app_private  = NULL;
+
+    return 0;
+
+}
+
+static int config_enc_params(EbSvtAv1EncConfiguration *param,
+                             AVCodecContext *avctx)
+{
+    SvtContext *svt_enc = avctx->priv_data;
+    const AVPixFmtDescriptor *desc;
+    int ret;
+
+    param->source_width     = avctx->width;
+    param->source_height    = avctx->height;
+
+    desc = av_pix_fmt_desc_get(avctx->pix_fmt);
+    param->encoder_bit_depth = desc->comp[0].depth;
+    av_log(avctx, AV_LOG_DEBUG , "Encoder %d bits depth input\n", param->encoder_bit_depth);
+
+    if (desc->log2_chroma_w == 1 && desc->log2_chroma_h == 1)
+        param->encoder_color_format   = EB_YUV420;
+    else if (desc->log2_chroma_w == 1 && desc->log2_chroma_h == 0)
+        param->encoder_color_format   = EB_YUV422;
+    else if (!desc->log2_chroma_w && !desc->log2_chroma_h)
+        param->encoder_color_format   = EB_YUV444;
+    else {
+        av_log(avctx, AV_LOG_ERROR , "Unsupported pixel format\n");
+        return AVERROR(EINVAL);
+    }
+    av_log(avctx, AV_LOG_DEBUG , "Encoder color format is %d \n", param->encoder_color_format);
+
+    param->profile = svt_enc->profile;
+
+    if ((param->encoder_color_format == EB_YUV422 || param->encoder_bit_depth > 10)
+         && param->profile != PROFESSIONAL_PROFILE ) {
+        av_log(avctx, AV_LOG_WARNING, "Force to be professional profile \n");
+        param->profile = PROFESSIONAL_PROFILE;
+    } else if (param->encoder_color_format == EB_YUV444 && param->profile != HIGH_PROFILE) {
+        av_log(avctx, AV_LOG_WARNING, "Force to be high profile \n");
+        param->profile = HIGH_PROFILE;
+    }
+
+    // Update param from options
+    param->hierarchical_levels     = svt_enc->hierarchical_level;
+    param->enc_mode                = svt_enc->enc_mode;
+    param->tier                   = svt_enc->tier;
+    param->level                  = svt_enc->level;
+    param->rate_control_mode        = svt_enc->rc_mode;
+    param->scene_change_detection   = svt_enc->scd;
+    param->base_layer_switch_mode    = svt_enc->base_layer_switch_mode;
+    param->qp                     = svt_enc->qp;
+
+
+    param->target_bit_rate          = avctx->bit_rate;
+    if (avctx->gop_size > 0)
+        param->intra_period_length  = avctx->gop_size - 1;
+
+    if (avctx->framerate.num > 0 && avctx->framerate.den > 0) {
+        param->frame_rate_numerator     = avctx->framerate.num;
+        param->frame_rate_denominator   = avctx->framerate.den * avctx->ticks_per_frame;
+    } else {
+        param->frame_rate_numerator     = avctx->time_base.den;
+        param->frame_rate_denominator   = avctx->time_base.num * avctx->ticks_per_frame;
+    }
+
+    if (param->rate_control_mode) {
+        param->max_qp_allowed       = avctx->qmax;
+        param->min_qp_allowed       = avctx->qmin;
+    }
+
+    param->intra_refresh_type       = svt_enc->forced_idr + 1;
+
+    if (svt_enc->la_depth != -1)
+        param->look_ahead_distance  = svt_enc->la_depth;
+
+    return 0;
+}
+
+static void read_in_data(const AVFrame *frame,
+                         EbBufferHeaderType *header_ptr)
+{
+    EbSvtIOFormat *in_data = (EbSvtIOFormat *)header_ptr->p_buffer;
+    const AVPixFmtDescriptor *desc;
+    int i, bytes_shift, plane_h;
+
+    desc = av_pix_fmt_desc_get(frame->format);
+    bytes_shift = desc->comp[0].depth > 8 ? 1 : 0;
+
+    in_data->luma = frame->data[0];
+    in_data->cb   = frame->data[1];
+    in_data->cr   = frame->data[2];
+
+    in_data->y_stride  = AV_CEIL_RSHIFT(frame->linesize[0], bytes_shift);
+    in_data->cb_stride = AV_CEIL_RSHIFT(frame->linesize[1], bytes_shift);
+    in_data->cr_stride = AV_CEIL_RSHIFT(frame->linesize[2], bytes_shift);
+
+    for (i = 0; i < desc->nb_components; i++) {
+        plane_h = frame->height;
+        if (i > 0)
+            plane_h = AV_CEIL_RSHIFT(plane_h, desc->log2_chroma_h);
+        header_ptr->n_filled_len += frame->linesize[i] * plane_h;
+    }
+}
+
+static av_cold int eb_enc_init(AVCodecContext *avctx)
+{
+    SvtContext   *svt_enc = avctx->priv_data;
+    EbErrorType svt_ret;
+    int ret;
+
+    svt_enc->eos_flag = EOS_NOT_REACHED;
+
+    svt_ret = eb_init_handle(&svt_enc->svt_handle, svt_enc, &svt_enc->enc_params);
+    if (svt_ret != EB_ErrorNone) {
+        return svt_print_error(avctx, svt_ret, "Error init encoder handle");
+    }
+
+    ret = config_enc_params(&svt_enc->enc_params, avctx);
+    if (ret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Error configure encoder parameters\n");
+        return ret;
+    }
+
+    svt_ret = eb_svt_enc_set_parameter(svt_enc->svt_handle, &svt_enc->enc_params);
+    if (svt_ret != EB_ErrorNone) {
+        return svt_print_error(avctx, svt_ret, "Error setting encoder parameters");
+    }
+
+    svt_ret = eb_init_encoder(svt_enc->svt_handle);
+    if (svt_ret != EB_ErrorNone) {
+        eb_deinit_handle(svt_enc->svt_handle);
+        svt_enc->svt_handle = NULL;
+        return svt_print_error(avctx, svt_ret, "Error init encoder");
+    }
+
+    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {
+        EbBufferHeaderType *headerPtr = NULL;
+
+        svt_ret = eb_svt_enc_stream_header(svt_enc->svt_handle, &headerPtr);
+        if (svt_ret != EB_ErrorNone) {
+            return svt_print_error(avctx, svt_ret, "Error when build stream header");
+        }
+
+        avctx->extradata_size = headerPtr->n_filled_len;
+        avctx->extradata = av_mallocz(avctx->extradata_size + AV_INPUT_BUFFER_PADDING_SIZE);
+        if (!avctx->extradata) {
+            av_log(avctx, AV_LOG_ERROR,
+                   "Cannot allocate AV1 header of size %d.\n", avctx->extradata_size);
+            return AVERROR(ENOMEM);
+        }
+
+        memcpy(avctx->extradata, headerPtr->p_buffer, avctx->extradata_size);
+
+        svt_ret = eb_svt_release_enc_stream_header(headerPtr);
+        if (svt_ret != EB_ErrorNone) {
+            return svt_print_error(avctx, svt_ret, "Error when destroy stream header");
+        }
+    }
+
+    ret = alloc_buffer(&svt_enc->enc_params, svt_enc);
+    if (ret < 0)
+        return ret;
+    return 0;
+
+}
+
+static int eb_send_frame(AVCodecContext *avctx, const AVFrame *frame)
+{
+    SvtContext           *svt_enc = avctx->priv_data;
+    EbBufferHeaderType  *headerPtr = svt_enc->in_buf;
+
+    if (!frame) {
+        EbBufferHeaderType headerPtrLast;
+        headerPtrLast.n_alloc_len   = 0;
+        headerPtrLast.n_filled_len  = 0;
+        headerPtrLast.n_tick_count  = 0;
+        headerPtrLast.p_app_private = NULL;
+        headerPtrLast.p_buffer     = NULL;
+        headerPtrLast.flags      = EB_BUFFERFLAG_EOS;
+
+        eb_svt_enc_send_picture(svt_enc->svt_handle, &headerPtrLast);
+        svt_enc->eos_flag = EOS_SENT;
+        av_log(avctx, AV_LOG_DEBUG, "Finish sending frames!!!\n");
+        return 0;
+    }
+
+    read_in_data(frame, headerPtr);
+
+    headerPtr->flags       = 0;
+    headerPtr->p_app_private  = NULL;
+    headerPtr->pts          = frame->pts;
+
+    eb_svt_enc_send_picture(svt_enc->svt_handle, headerPtr);
+
+    return 0;
+}
+
+static int eb_receive_packet(AVCodecContext *avctx, AVPacket *pkt)
+{
+    SvtContext  *svt_enc = avctx->priv_data;
+    EbBufferHeaderType *headerPtr;
+    EbErrorType svt_ret;
+    int ret = 0, pict_type;
+
+    if (svt_enc->eos_flag == EOS_RECEIVED)
+        return AVERROR_EOF;
+
+    if ((ret = ff_alloc_packet2(avctx, pkt, svt_enc->raw_size, 0)) < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Failed to allocate output packet.\n");
+        return ret;
+    }
+    svt_ret = eb_svt_get_packet(svt_enc->svt_handle, &headerPtr, svt_enc->eos_flag);
+    if (svt_ret == EB_NoErrorEmptyQueue)
+        return AVERROR(EAGAIN);
+
+    memcpy(pkt->data, headerPtr->p_buffer, headerPtr->n_filled_len);
+    pkt->size = headerPtr->n_filled_len;
+    pkt->pts  = headerPtr->pts;
+    pkt->dts  = headerPtr->dts;
+    if (headerPtr->pic_type == EB_AV1_KEY_PICTURE) {
+        pkt->flags |= AV_PKT_FLAG_KEY;
+        pict_type = AV_PICTURE_TYPE_I;
+    } else if (headerPtr->pic_type == EB_AV1_INTRA_ONLY_PICTURE) {
+        pict_type = AV_PICTURE_TYPE_I;
+    } else if (headerPtr->pic_type == EB_AV1_INVALID_PICTURE) {
+        pict_type = AV_PICTURE_TYPE_NONE;
+    } else
+        pict_type = AV_PICTURE_TYPE_P;
+
+    if (headerPtr->pic_type == EB_AV1_NON_REF_PICTURE)
+        pkt->flags |= AV_PKT_FLAG_DISPOSABLE;
+
+    if (headerPtr->flags & EB_BUFFERFLAG_EOS)
+        svt_enc->eos_flag = EOS_RECEIVED;
+
+    ff_side_data_set_encoder_stats(pkt, headerPtr->qp * FF_QP2LAMBDA, NULL, 0, pict_type);
+
+    eb_svt_release_out_buffer(&headerPtr);
+
+    return ret;
+}
+
+static av_cold int eb_enc_close(AVCodecContext *avctx)
+{
+    SvtContext *svt_enc = avctx->priv_data;
+
+    if (svt_enc) {
+        if (svt_enc->svt_handle) {
+            eb_deinit_encoder(svt_enc->svt_handle);
+            eb_deinit_handle(svt_enc->svt_handle);
+        }
+
+        free_buffer(svt_enc);
+        svt_enc = NULL;
+    }
+    return 0;
+}
+
+#define OFFSET(x) offsetof(SvtContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+static const AVOption options[] = {
+    { "hielevel", "Hierarchical prediction levels setting", OFFSET(hierarchical_level),
+      AV_OPT_TYPE_INT, { .i64 = 4 }, 3, 4, VE , "hielevel"},
+        { "3level", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = 3 },  INT_MIN, INT_MAX, VE, "hielevel" },
+        { "4level", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = 4 },  INT_MIN, INT_MAX, VE, "hielevel" },
+
+    { "la_depth", "Look ahead distance [0, 120]", OFFSET(la_depth),
+      AV_OPT_TYPE_INT, { .i64 = -1 }, -1, 120, VE },
+
+    { "preset", "Encoding preset [0, 8]",
+      OFFSET(enc_mode), AV_OPT_TYPE_INT, { .i64 = MAX_ENC_PRESET }, 0, MAX_ENC_PRESET, VE },
+
+    { "profile", "Set profile restrictions", OFFSET(profile), AV_OPT_TYPE_INT, { .i64 = MAIN_PROFILE}, MAIN_PROFILE, PROFESSIONAL_PROFILE, VE, "profile" },
+    { "main" , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = MAIN_PROFILE}, INT_MIN, INT_MAX,     VE, "profile" },
+    { "high" , NULL, 0, AV_OPT_TYPE_CONST, { .i64 = HIGH_PROFILE}, INT_MIN, INT_MAX,     VE, "profile" },
+    { "professional", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = PROFESSIONAL_PROFILE }, INT_MIN, INT_MAX,     VE, "profile" },
+
+    { "tier", "Set tier (general_tier_flag)", OFFSET(tier),
+      AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 1, VE, "tier" },
+        { "main", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = 0 }, 0, 0, VE, "tier" },
+        { "high", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = 1 }, 0, 0, VE, "tier" },
+
+    { "level", "Set level (level_idc)", OFFSET(level),
+      AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 0x1f, VE, "level" },
+
+#define LEVEL(name, value) name, NULL, 0, AV_OPT_TYPE_CONST, \
+      { .i64 = value }, 0, 0, VE, "level"
+        { LEVEL("2.0", 20) },
+        { LEVEL("2.1", 21) },
+        { LEVEL("2.2", 22) },
+        { LEVEL("2.3", 23) },
+        { LEVEL("3.0", 30) },
+        { LEVEL("3.1", 31) },
+        { LEVEL("3.2", 32) },
+        { LEVEL("3.3", 33) },
+        { LEVEL("4.0", 40) },
+        { LEVEL("4.1", 41) },
+        { LEVEL("4.2", 42) },
+        { LEVEL("4.3", 43) },
+        { LEVEL("5.0", 50) },
+        { LEVEL("5.1", 51) },
+        { LEVEL("5.2", 52) },
+        { LEVEL("5.3", 53) },
+        { LEVEL("6.0", 60) },
+        { LEVEL("6.1", 61) },
+        { LEVEL("6.2", 62) },
+        { LEVEL("6.3", 63) },
+        { LEVEL("7.0", 70) },
+        { LEVEL("7.1", 71) },
+        { LEVEL("7.2", 72) },
+        { LEVEL("7.3", 73) },
+#undef LEVEL
+
+    { "rc", "Bit rate control mode", OFFSET(rc_mode),
+      AV_OPT_TYPE_INT, { .i64 = 0 }, 0, 3, VE , "rc"},
+        { "cqp", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = 0 },  INT_MIN, INT_MAX, VE, "rc" },
+        { "vbr", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = 2 },  INT_MIN, INT_MAX, VE, "rc" },
+        { "cvbr", NULL, 0, AV_OPT_TYPE_CONST,{ .i64 = 3 },  INT_MIN, INT_MAX, VE, "rc" },
+
+    { "qp", "QP value for intra frames", OFFSET(qp),
+      AV_OPT_TYPE_INT, { .i64 = 50 }, 0, 63, VE },
+
+    { "sc_detection", "Scene change detection", OFFSET(scd),
+      AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VE },
+
+    { "bl_mode", "Random Access Prediction Structure type setting", OFFSET(base_layer_switch_mode),
+      AV_OPT_TYPE_BOOL, { .i64 = 0 }, 0, 1, VE },
+
+    { "forced-idr", "If forcing keyframes, force them as IDR frames.", OFFSET(forced_idr),
+      AV_OPT_TYPE_BOOL,   { .i64 = 0 }, 0, 1, VE },
+
+    {NULL},
+};
+
+static const AVClass class = {
+    .class_name = "libsvt_av1",
+    .item_name  = av_default_item_name,
+    .option     = options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+static const AVCodecDefault eb_enc_defaults[] = {
+    { "b",         "7M"    },
+    { "g",         "-2"    },
+    { "qmin",      "0"     },
+    { "qmax",      "63"    },
+    { NULL },
+};
+
+AVCodec ff_libsvt_av1_encoder = {
+    .name           = "libsvt_av1",
+    .long_name      = NULL_IF_CONFIG_SMALL("SVT-AV1(Scalable Video Technology for AV1) encoder"),
+    .priv_data_size = sizeof(SvtContext),
+    .type           = AVMEDIA_TYPE_VIDEO,
+    .id             = AV_CODEC_ID_AV1,
+    .init           = eb_enc_init,
+    .send_frame     = eb_send_frame,
+    .receive_packet = eb_receive_packet,
+    .close          = eb_enc_close,
+    .capabilities   = AV_CODEC_CAP_DELAY | AV_CODEC_CAP_AUTO_THREADS,
+    .pix_fmts       = (const enum AVPixelFormat[]){ AV_PIX_FMT_YUV420P,
+                                                    AV_PIX_FMT_YUV420P10,
+                                                    AV_PIX_FMT_NONE },
+    .priv_class     = &class,
+    .defaults       = eb_enc_defaults,
+    .caps_internal  = FF_CODEC_CAP_INIT_CLEANUP,
+    .wrapper_name   = "libsvt_av1",
+};
diff --git a/libavcodec/libx264.c b/libavcodec/libx264.c
index dc4b4b100d..cedcdef67b 100644
--- a/libavcodec/libx264.c
+++ b/libavcodec/libx264.c
@@ -195,24 +195,73 @@ static void reconfig_encoder(AVCodecContext *ctx, const AVFrame *frame)
         x264_encoder_reconfig(x4->enc, &x4->params);
     }
 
-    if (x4->params.rc.i_vbv_buffer_size != ctx->rc_buffer_size / 1000 ||
-        x4->params.rc.i_vbv_max_bitrate != ctx->rc_max_rate    / 1000) {
-        x4->params.rc.i_vbv_buffer_size = ctx->rc_buffer_size / 1000;
-        x4->params.rc.i_vbv_max_bitrate = ctx->rc_max_rate    / 1000;
-        x264_encoder_reconfig(x4->enc, &x4->params);
+    if (frame->perceptual_score == -1) {
+        if (x4->params.rc.i_vbv_buffer_size != ctx->rc_buffer_size / 1000 ||
+            x4->params.rc.i_vbv_max_bitrate != ctx->rc_max_rate    / 1000) {
+            x4->params.rc.i_vbv_buffer_size = ctx->rc_buffer_size / 1000;
+            x4->params.rc.i_vbv_max_bitrate = ctx->rc_max_rate    / 1000;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        }
     }
 
     if (x4->params.rc.i_rc_method == X264_RC_ABR &&
-        x4->params.rc.i_bitrate != ctx->bit_rate / 1000) {
-        x4->params.rc.i_bitrate = ctx->bit_rate / 1000;
-        x264_encoder_reconfig(x4->enc, &x4->params);
+        (frame->perceptual_score > -1 ||
+         x4->params.rc.i_bitrate != ctx->bit_rate / 1000)) {
+        if (frame->perceptual_score > -1) {
+            int bitrate = 0;
+            /* set ABR bitrate value from perceptual score */
+            /* decrease compression by raising the avg bitrate up to N times */
+            bitrate = (ctx->bit_rate / 1000) + ((frame->perceptual_score * frame->perceptual_score_factor) * (ctx->bit_rate / 1000.0));
+            x4->params.rc.i_bitrate = bitrate;
+            x4->params.rc.i_vbv_max_bitrate = bitrate * 1.5;
+            x4->params.rc.i_vbv_buffer_size = bitrate * 1.5 * 1.5;
+            av_log(ctx, AV_LOG_DEBUG,
+               "Perceptual: bitrate %d maxbitrate %d from %"PRIu64"\n", 
+               x4->params.rc.i_bitrate, 
+               x4->params.rc.i_vbv_max_bitrate,
+               ctx->bit_rate / 1000);
+
+            /* tag this frame with this specific config */
+            x4->pic.param = &x4->params;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        } else {
+            x4->params.rc.i_bitrate = ctx->bit_rate / 1000;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        }
     }
 
     if (x4->crf >= 0 &&
         x4->params.rc.i_rc_method == X264_RC_CRF &&
-        x4->params.rc.f_rf_constant != x4->crf) {
-        x4->params.rc.f_rf_constant = x4->crf;
-        x264_encoder_reconfig(x4->enc, &x4->params);
+        (frame->perceptual_score > -1 ||
+         x4->params.rc.f_rf_constant != x4->crf)) {
+        if (frame->perceptual_score > -1) {
+            float crf_value = 0.0;
+            if (ctx->rc_max_rate) {
+                int bitrate = 0;
+                /* set crf value from perceptual score */
+                /* decrease compression by lowering the score by up to N CRF points */
+                crf_value = x4->crf - ((frame->perceptual_score * 100.0) / (frame->perceptual_score_factor * 2));
+                x4->params.rc.f_rf_constant = crf_value;
+
+                /* set ABR bitrate value from perceptual score */
+                /* decrease compression by raising the avg bitrate up to N times */
+                bitrate = (ctx->rc_max_rate / 1000) + ((frame->perceptual_score * frame->perceptual_score_factor) * (ctx->rc_max_rate / 1000.0));
+                x4->params.rc.i_vbv_max_bitrate = bitrate;
+                x4->params.rc.i_vbv_buffer_size = bitrate * 1.5 * 1.5;
+            }
+            av_log(ctx, AV_LOG_DEBUG,
+               "Perceptual: crf: %0.2f bitrate %d maxbitrate %d from %"PRIu64"\n", 
+               x4->params.rc.f_rf_constant,
+               x4->params.rc.i_bitrate, 
+               x4->params.rc.i_vbv_max_bitrate,
+               ctx->rc_max_rate / 1000);
+
+            /* tag this frame with this specific config */
+            x4->pic.param = &x4->params;
+        } else {
+            x4->params.rc.f_rf_constant = x4->crf;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        }
     }
 
     if (x4->params.rc.i_rc_method == X264_RC_CQP &&
diff --git a/libavfilter/Makefile b/libavfilter/Makefile
index 455c809b15..a51087f28f 100644
--- a/libavfilter/Makefile
+++ b/libavfilter/Makefile
@@ -307,6 +307,8 @@ OBJS-$(CONFIG_NORMALIZE_FILTER)              += vf_normalize.o
 OBJS-$(CONFIG_NULL_FILTER)                   += vf_null.o
 OBJS-$(CONFIG_OCR_FILTER)                    += vf_ocr.o
 OBJS-$(CONFIG_OCV_FILTER)                    += vf_libopencv.o
+OBJS-$(CONFIG_PERCEPTUAL_FILTER)             += vf_perceptual.o img_hash.o
+OBJS-$(CONFIG_IMG_HASH_FILTER)               += vf_img_hash.o img_hash.o
 OBJS-$(CONFIG_OSCILLOSCOPE_FILTER)           += vf_datascope.o
 OBJS-$(CONFIG_OVERLAY_FILTER)                += vf_overlay.o framesync.o
 OBJS-$(CONFIG_OVERLAY_OPENCL_FILTER)         += vf_overlay_opencl.o opencl.o \
@@ -489,6 +491,7 @@ OBJS-$(CONFIG_SHARED)                        += log2_tab.o
 SKIPHEADERS-$(CONFIG_QSVVPP)                 += qsvvpp.h
 SKIPHEADERS-$(CONFIG_OPENCL)                 += opencl.h
 SKIPHEADERS-$(CONFIG_VAAPI)                  += vaapi_vpp.h
+SKIPHEADERS-$(CONFIG_LIBOPENCV)              += img_hash.h
 
 TOOLS     = graph2dot
 TESTPROGS = drawutils filtfmts formats integral
diff --git a/libavfilter/allfilters.c b/libavfilter/allfilters.c
index 04a3df7d56..ab565634e4 100644
--- a/libavfilter/allfilters.c
+++ b/libavfilter/allfilters.c
@@ -292,6 +292,8 @@ extern AVFilter ff_vf_normalize;
 extern AVFilter ff_vf_null;
 extern AVFilter ff_vf_ocr;
 extern AVFilter ff_vf_ocv;
+extern AVFilter ff_vf_perceptual;
+extern AVFilter ff_vf_img_hash;
 extern AVFilter ff_vf_oscilloscope;
 extern AVFilter ff_vf_overlay;
 extern AVFilter ff_vf_overlay_opencl;
diff --git a/libavfilter/img_hash.cpp b/libavfilter/img_hash.cpp
new file mode 100644
index 0000000000..e6bf32f982
--- /dev/null
+++ b/libavfilter/img_hash.cpp
@@ -0,0 +1,86 @@
+// Christopher Kennedy 2019
+// License: Apache 2.0
+//
+// Originally inspired by the information from:
+// https://qtandopencv.blogspot.com/2016/06/introduction-to-image-hash-module-of.html
+//
+#include <opencv2/core.hpp>
+#include <opencv2/core/ocl.hpp>
+#include <opencv2/highgui.hpp>
+#include <opencv2/img_hash.hpp>
+#include <opencv2/imgproc.hpp>
+
+#include <iostream>
+
+#include "img_hash.h"
+#include "libavutil/pixdesc.h"
+extern "C" {
+#include "avfilter.h"
+}
+
+using namespace cv;
+using namespace cv::img_hash;
+using namespace std;
+
+// From the libopencv AVFilter
+static void fill_iplimage_from_frame(IplImage *img, const AVFrame *frame, enum AVPixelFormat pixfmt)
+{
+    IplImage *tmpimg;
+    int depth, channels_nb;
+
+    if      (pixfmt == AV_PIX_FMT_GRAY8) { depth = IPL_DEPTH_8U;  channels_nb = 1; }
+    else if (pixfmt == AV_PIX_FMT_BGRA)  { depth = IPL_DEPTH_8U;  channels_nb = 4; }
+    else if (pixfmt == AV_PIX_FMT_BGR24) { depth = IPL_DEPTH_8U;  channels_nb = 3; }
+    else if (pixfmt == AV_PIX_FMT_YUV420P) { depth = IPL_DEPTH_8U;  channels_nb = 3; }
+    else return;
+
+    tmpimg = cvCreateImageHeader((CvSize){frame->width, frame->height}, depth, channels_nb);
+    *img = *tmpimg;
+    img->imageData = img->imageDataOrigin = (char *) frame->data[0];
+    img->dataOrder = IPL_DATA_ORDER_PIXEL;
+    img->origin    = IPL_ORIGIN_TL;
+    img->widthStep = frame->linesize[0];
+}
+
+// Get the score of two Video Frames by comparing the perceptual hashes and deriving a hamming distance
+// showing how similar they are or different. lower the score is better for most algorithms
+extern "C" double getScore(const AVFrame *frame1, const AVFrame *frame2, enum AVPixelFormat pixfmt, int hash_type) {
+    cv::Ptr<cv::img_hash::ImgHashBase> algo;
+    IplImage ipl1, ipl2;
+    cv::Mat h1;
+    cv::Mat h2;
+    cv::Mat m1;
+    cv::Mat m2;
+
+    // Take FFmpeg video frame and convert into an IplImage for OpenCV
+    fill_iplimage_from_frame(&ipl1, frame1, pixfmt);
+    fill_iplimage_from_frame(&ipl2, frame2, pixfmt);
+    // Convert an IplImage to an Mat Image for OpenCV (newer format)
+    m1 = cv::cvarrToMat(&ipl1);
+    m2 = cv::cvarrToMat(&ipl2);
+
+    // substantiate the hash type algorithm
+    if (hash_type == COLORMOMENT) {
+        algo = cv::img_hash::ColorMomentHash::create();
+    } else if (hash_type == AVERAGE) {
+        algo = cv::img_hash::AverageHash::create();
+    } else if (hash_type == BLOCKMEAN1) {
+        //BlockMeanHash support mode 0 and mode 1, they associate to 
+        //    //mode 1 and mode 2 of PHash library
+        algo = cv::img_hash::BlockMeanHash::create(0);
+    } else if (hash_type == BLOCKMEAN2) {
+        algo = cv::img_hash::BlockMeanHash::create(1);
+    } else if (hash_type == MARRHILDRETH) {
+        algo = cv::img_hash::MarrHildrethHash::create();
+    } else if (hash_type == RADIALVARIANCE) {
+        algo = cv::img_hash::RadialVarianceHash::create();
+    } else { // Default to PHash
+        algo = cv::img_hash::PHash::create();
+    }
+    // Compute the hash
+    algo->compute(m1, h1);
+    algo->compute(m2, h2);
+    // Compare the hashes and return the hamming distance
+    return algo->compare(h1, h2);
+}
+
diff --git a/libavfilter/img_hash.h b/libavfilter/img_hash.h
new file mode 100644
index 0000000000..c916766647
--- /dev/null
+++ b/libavfilter/img_hash.h
@@ -0,0 +1,31 @@
+// Christopher Kennedy 2019
+// License: Apache 2.0
+//
+// Tutorial and code from
+// https://qtandopencv.blogspot.com/2016/06/introduction-to-image-hash-module-of.html
+//
+#ifndef AVFILTER_IMG_HASH_H
+#define AVFILTER_IMG_HASH_H
+
+#include "avfilter.h"
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+#define PHASH 0
+#define COLORMOMENT 1
+#define AVERAGE 2
+#define MARRHILDRETH 3
+#define RADIALVARIANCE 4
+#define BLOCKMEAN1 5
+#define BLOCKMEAN2 6
+
+double getScore(const AVFrame *frame1, const AVFrame *frame2, enum AVPixelFormat pixfmt, int hash_type);
+#ifdef __cplusplus
+}
+#endif
+
+
+#endif
diff --git a/libavfilter/vf_img_hash.c b/libavfilter/vf_img_hash.c
new file mode 100644
index 0000000000..f35010d088
--- /dev/null
+++ b/libavfilter/vf_img_hash.c
@@ -0,0 +1,513 @@
+/*
+ * Copyright (c) 2019 Christopher Kennedy
+ *
+ * PHQM Perceptual Hash Quality Metric
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * PHQM: Caculate the Image Hash Hamming Difference between two input videos.
+ */
+
+#include "libavutil/avstring.h"
+#include "libavutil/opt.h"
+#include "libavutil/pixdesc.h"
+#include "avfilter.h"
+#include "drawutils.h"
+#include "formats.h"
+#include "framesync.h"
+#include "internal.h"
+#include "psnr.h"
+#include "video.h"
+
+#include "img_hash.h"
+#include "scene_sad.h"
+
+typedef struct ImgHashContext {
+    const AVClass *class;
+    FFFrameSync fs;
+    double shd, hd, min_hd, max_hd, smin_hd, smax_hd, mse, min_mse, max_mse, mse_comp[4];
+    uint64_t nb_shd;
+    uint64_t nb_frames;
+    FILE *stats_file;
+    char *stats_file_str;
+    int stats_version;
+    char *hash_type;
+    int hash_type_i;
+    int stats_header_written;
+    int stats_add_max;
+    int max[4], average_max;
+    int is_rgb;
+    uint8_t rgba_map[4];
+    char comps[4];
+    int nb_components;
+    int planewidth[4];
+    int planeheight[4];
+    double planeweight[4];
+    PSNRDSPContext dsp;
+    ff_scene_sad_fn sad;            ///< Sum of the absolute difference function (scene detect only)
+    double prev_mafd;               ///< previous MAFD                           (scene detect only)
+    AVFrame *prev_picref;           ///< previous frame                          (scene detect only)
+    double scd_thresh;
+    double scene_score;
+} ImgHashContext;
+
+#define OFFSET(x) offsetof(ImgHashContext, x)
+#define FLAGS AV_OPT_FLAG_FILTERING_PARAM|AV_OPT_FLAG_VIDEO_PARAM
+
+static const AVOption img_hash_options[] = {
+    {"stats_file", "Set file where to store per-frame difference information", OFFSET(stats_file_str), AV_OPT_TYPE_STRING, {.str=NULL}, 0, 0, FLAGS },
+    {"f",          "Set file where to store per-frame difference information", OFFSET(stats_file_str), AV_OPT_TYPE_STRING, {.str=NULL}, 0, 0, FLAGS },
+    {"stats_version", "Set the format version for the stats file.",               OFFSET(stats_version),  AV_OPT_TYPE_INT,    {.i64=1},    1, 2, FLAGS },
+    {"scd_thresh", "Scene Change Detection Threshold. 0.5 default, 0.0-1.0",    OFFSET(scd_thresh),  AV_OPT_TYPE_DOUBLE,    {.dbl=0.5},    0, 1, FLAGS },
+    {"output_max",  "Add raw stats (max values) to the output log.",            OFFSET(stats_add_max), AV_OPT_TYPE_BOOL, {.i64=0}, 0, 1, FLAGS},
+    { "hash_type", "options: phash, colormoment, average", OFFSET(hash_type), AV_OPT_TYPE_STRING, {.str = "phash"}, .flags = FLAGS },
+    { NULL }
+};
+
+FRAMESYNC_DEFINE_CLASS(img_hash, ImgHashContext, fs);
+
+static inline unsigned pow_2(unsigned base)
+{
+    return base*base;
+}
+
+static inline double get_psnr(double mse, uint64_t nb_frames, int max)
+{
+    return 10.0 * log10(pow_2(max) / (mse / nb_frames));
+}
+
+static uint64_t sse_line_8bit(const uint8_t *main_line,  const uint8_t *ref_line, int outw)
+{
+    int j;
+    unsigned m2 = 0;
+
+    for (j = 0; j < outw; j++)
+        m2 += pow_2(main_line[j] - ref_line[j]);
+
+    return m2;
+}
+
+static uint64_t sse_line_16bit(const uint8_t *_main_line, const uint8_t *_ref_line, int outw)
+{
+    int j;
+    uint64_t m2 = 0;
+    const uint16_t *main_line = (const uint16_t *) _main_line;
+    const uint16_t *ref_line = (const uint16_t *) _ref_line;
+
+    for (j = 0; j < outw; j++)
+        m2 += pow_2(main_line[j] - ref_line[j]);
+
+    return m2;
+}
+
+static inline
+void compute_images_mse(ImgHashContext *s,
+                        const uint8_t *main_data[4], const int main_linesizes[4],
+                        const uint8_t *ref_data[4], const int ref_linesizes[4],
+                        int w, int h, double mse[4])
+{
+    int i, c;
+
+    for (c = 0; c < s->nb_components; c++) {
+        const int outw = s->planewidth[c];
+        const int outh = s->planeheight[c];
+        const uint8_t *main_line = main_data[c];
+        const uint8_t *ref_line = ref_data[c];
+        const int ref_linesize = ref_linesizes[c];
+        const int main_linesize = main_linesizes[c];
+        uint64_t m = 0;
+        for (i = 0; i < outh; i++) {
+            m += s->dsp.sse_line(main_line, ref_line, outw);
+            ref_line += ref_linesize;
+            main_line += main_linesize;
+        }
+        mse[c] = m / (double)(outw * outh);
+    }
+}
+
+static void set_meta(AVDictionary **metadata, const char *key, char comp, float d)
+{
+    char value[128];
+    snprintf(value, sizeof(value), "%0.2f", d);
+    if (comp) {
+        char key2[128];
+        snprintf(key2, sizeof(key2), "%s%c", key, comp);
+        av_dict_set(metadata, key2, value, 0);
+    } else {
+        av_dict_set(metadata, key, value, 0);
+    }
+}
+
+static double get_scene_score(AVFilterContext *ctx, AVFrame *frame)
+{
+    double ret = 0;
+    ImgHashContext *s = ctx->priv;
+    AVFrame *prev_picref = s->prev_picref;
+
+    if (prev_picref &&
+        frame->height == prev_picref->height &&
+        frame->width  == prev_picref->width) {
+        uint64_t sad;
+        double mafd, diff;
+
+        s->sad(prev_picref->data[0], prev_picref->linesize[0], frame->data[0], frame->linesize[0], frame->width * 3, frame->height, &sad);
+        emms_c();
+        mafd = (double)sad / (frame->width * 3 * frame->height);
+        diff = fabs(mafd - s->prev_mafd);
+        ret  = av_clipf(FFMIN(mafd, diff) / 100., 0, 1);
+        s->prev_mafd = mafd;
+        av_frame_free(&prev_picref);
+    }
+    s->prev_picref = av_frame_clone(frame);
+    return ret;
+}
+
+static int do_phqm(FFFrameSync *fs)
+{
+    AVFilterContext *ctx = fs->parent;
+    ImgHashContext *s = ctx->priv;
+    AVFrame *master, *ref;
+    double comp_mse[4], mse = 0, hd = 0;
+    int ret, j, c;
+    double hd_limit = 1000000;
+    AVDictionary **metadata;
+
+    ret = ff_framesync_dualinput_get(fs, &master, &ref);
+    if (ret < 0)
+        return ret;
+    if (!ref)
+        return ff_filter_frame(ctx->outputs[0], master);
+    metadata = &master->metadata;
+
+    compute_images_mse(s, (const uint8_t **)master->data, master->linesize,
+                          (const uint8_t **)ref->data, ref->linesize,
+                          master->width, master->height, comp_mse);
+
+    for (j = 0; j < s->nb_components; j++)
+        mse += comp_mse[j] * s->planeweight[j];
+
+    s->min_mse = FFMIN(s->min_mse, mse);
+    s->max_mse = FFMAX(s->max_mse, mse);
+
+    s->mse += mse;
+    for (j = 0; j < s->nb_components; j++)
+        s->mse_comp[j] += comp_mse[j];
+    s->nb_frames++;
+
+    for (j = 0; j < s->nb_components; j++) {
+        c = s->is_rgb ? s->rgba_map[j] : j;
+        set_meta(metadata, "lavfi.psnr.mse.", s->comps[j], comp_mse[c]);
+        set_meta(metadata, "lavfi.psnr.psnr.", s->comps[j], get_psnr(comp_mse[c], 1, s->max[c]));
+    }
+    set_meta(metadata, "lavfi.psnr.mse_avg", 0, mse);
+    set_meta(metadata, "lavfi.psnr.psnr_avg", 0, get_psnr(mse, 1, s->average_max));
+
+    /* scene change detection score */
+    s->scene_score = get_scene_score(ctx, ref);
+    if (s->scene_score >= s->scd_thresh && s->nb_shd >= 48) {
+        av_log(s, AV_LOG_WARNING, "ImgHashScene: n:%"PRId64"-%"PRId64" hd_avg:%0.3lf hd_min:%0.3lf hd_max:%0.3lf scd:%0.2lf\n",
+               (s->nb_frames - s->nb_shd), s->nb_frames - 1, (s->shd / s->nb_shd), s->smin_hd, s->smax_hd, s->scene_score);
+        s->shd = 0;
+        s->nb_shd = 0;
+        s->smin_hd = 0;
+        s->smax_hd = 0;
+    }
+
+    /* limit the highest value so we cut off at perceptual difference match */
+    if (s->hash_type_i == PHASH || s->hash_type_i == AVERAGE)
+        hd_limit = 5;
+    else if (s->hash_type_i == MARRHILDRETH)
+        hd_limit = 30;
+    else if (s->hash_type_i == RADIALVARIANCE)
+        hd_limit = 0.9;
+    else if (s->hash_type_i == BLOCKMEAN1)
+        hd_limit = 12;
+    else if (s->hash_type_i == BLOCKMEAN2)
+        hd_limit = 48;
+    else if (s->hash_type_i == COLORMOMENT)
+        hd_limit = 8;
+
+    /* get ref / enc perceptual hashes and calc hamming distance difference value */
+    hd = getScore(ref, master, ref->format, s->hash_type_i);
+    s->hd += FFMIN(hd, hd_limit);
+
+    /* scene hamming distance avg */
+    s->shd += FFMIN(hd, hd_limit);
+    s->nb_shd++;
+    av_log(s, AV_LOG_DEBUG, "ImgHashFrame: hd:%0.3lf scd:%0.2lf\n", hd, s->scene_score);
+
+    s->min_hd = FFMIN(s->min_hd, hd);
+    s->max_hd = FFMAX(s->max_hd, hd);
+    s->smin_hd = FFMIN(s->smin_hd, hd);
+    s->smax_hd = FFMAX(s->smax_hd, hd);
+
+    if (s->stats_file) {
+        if (s->stats_version == 2 && !s->stats_header_written) {
+            fprintf(s->stats_file, "psnr_log_version:2 fields:n");
+            fprintf(s->stats_file, ",mse_avg");
+            for (j = 0; j < s->nb_components; j++) {
+                fprintf(s->stats_file, ",mse_%c", s->comps[j]);
+            }
+            fprintf(s->stats_file, ",psnr_avg");
+            for (j = 0; j < s->nb_components; j++) {
+                fprintf(s->stats_file, ",psnr_%c", s->comps[j]);
+            }
+            if (s->stats_add_max) {
+                fprintf(s->stats_file, ",max_avg");
+                for (j = 0; j < s->nb_components; j++) {
+                    fprintf(s->stats_file, ",max_%c", s->comps[j]);
+                }
+            }
+            fprintf(s->stats_file, "\n");
+            s->stats_header_written = 1;
+        }
+        fprintf(s->stats_file,
+                "n:%"PRId64" phqm_avg:%0.3f phqm_min:%0.3f phqm_max:%0.3f scd:%0.2f mse_avg:%0.2f ",
+                s->nb_frames, hd, s->min_hd, s->max_hd, s->scene_score, mse);
+        for (j = 0; j < s->nb_components; j++) {
+            c = s->is_rgb ? s->rgba_map[j] : j;
+            fprintf(s->stats_file, "mse_%c:%0.2f ", s->comps[j], comp_mse[c]);
+        }
+        fprintf(s->stats_file, "psnr_avg:%0.2f ", get_psnr(mse, 1, s->average_max));
+        for (j = 0; j < s->nb_components; j++) {
+            c = s->is_rgb ? s->rgba_map[j] : j;
+            fprintf(s->stats_file, "psnr_%c:%0.2f ", s->comps[j],
+                    get_psnr(comp_mse[c], 1, s->max[c]));
+        }
+        if (s->stats_version == 2 && s->stats_add_max) {
+            fprintf(s->stats_file, "max_avg:%d ", s->average_max);
+            for (j = 0; j < s->nb_components; j++) {
+                c = s->is_rgb ? s->rgba_map[j] : j;
+                fprintf(s->stats_file, "max_%c:%d ", s->comps[j], s->max[c]);
+            }
+        }
+        fprintf(s->stats_file, "\n");
+    }
+
+    return ff_filter_frame(ctx->outputs[0], master);
+}
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    ImgHashContext *s = ctx->priv;
+
+    s->min_mse = +INFINITY;
+    s->max_mse = -INFINITY;
+
+    if (s->stats_file_str) {
+        if (s->stats_version < 2 && s->stats_add_max) {
+            av_log(ctx, AV_LOG_ERROR,
+                "stats_add_max was specified but stats_version < 2.\n" );
+            return AVERROR(EINVAL);
+        }
+        if (!strcmp(s->stats_file_str, "-")) {
+            s->stats_file = stdout;
+        } else {
+            s->stats_file = fopen(s->stats_file_str, "w");
+            if (!s->stats_file) {
+                int err = AVERROR(errno);
+                char buf[128];
+                av_strerror(err, buf, sizeof(buf));
+                av_log(ctx, AV_LOG_ERROR, "Could not open stats file %s: %s\n",
+                       s->stats_file_str, buf);
+                return err;
+            }
+        }
+    }
+
+    if (s->hash_type) {
+        if (!strcmp(s->hash_type, "phash"))
+            s->hash_type_i = PHASH;
+        else if (!strcmp(s->hash_type, "colormoment")) {
+            s->hash_type_i = COLORMOMENT;
+        } else if (!strcmp(s->hash_type, "average"))
+            s->hash_type_i = AVERAGE;
+        else if (!strcmp(s->hash_type, "marrhildreth"))
+            s->hash_type_i = MARRHILDRETH;
+        else if (!strcmp(s->hash_type, "radialvariance"))
+            s->hash_type_i = RADIALVARIANCE;
+        else if (!strcmp(s->hash_type, "blockmean1"))
+            s->hash_type_i = BLOCKMEAN1;
+        else if (!strcmp(s->hash_type, "blockmean2"))
+            s->hash_type_i = BLOCKMEAN2;
+        else {
+            av_log(s, AV_LOG_ERROR, "Bad hash_type given %s\n", s->hash_type);
+            return AVERROR(EINVAL);
+        }
+    }
+
+    s->sad = ff_scene_sad_get_fn(8);
+    if (!s->sad)
+        return AVERROR(EINVAL);
+
+    s->fs.on_event = do_phqm;
+    return 0;
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_YUV420P, AV_PIX_FMT_BGR24, AV_PIX_FMT_GRAY8, AV_PIX_FMT_NONE
+    };
+    return ff_set_common_formats(ctx, ff_make_format_list(pix_fmts));
+}
+
+static int config_input_ref(AVFilterLink *inlink)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(inlink->format);
+    AVFilterContext *ctx  = inlink->dst;
+    ImgHashContext *s = ctx->priv;
+    double average_max;
+    unsigned sum;
+    int j;
+
+    s->nb_components = desc->nb_components;
+    if (ctx->inputs[0]->w != ctx->inputs[1]->w ||
+        ctx->inputs[0]->h != ctx->inputs[1]->h) {
+        av_log(ctx, AV_LOG_ERROR, "Width and height of input videos must be same.\n");
+        return AVERROR(EINVAL);
+    }
+    if (ctx->inputs[0]->format != ctx->inputs[1]->format) {
+        av_log(ctx, AV_LOG_ERROR, "Inputs must be of same pixel format.\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->max[0] = (1 << desc->comp[0].depth) - 1;
+    s->max[1] = (1 << desc->comp[1].depth) - 1;
+    s->max[2] = (1 << desc->comp[2].depth) - 1;
+    s->max[3] = (1 << desc->comp[3].depth) - 1;
+
+    s->is_rgb = ff_fill_rgba_map(s->rgba_map, inlink->format) >= 0;
+    s->comps[0] = s->is_rgb ? 'r' : 'y' ;
+    s->comps[1] = s->is_rgb ? 'g' : 'u' ;
+    s->comps[2] = s->is_rgb ? 'b' : 'v' ;
+    s->comps[3] = 'a';
+
+    s->planeheight[1] = s->planeheight[2] = AV_CEIL_RSHIFT(inlink->h, desc->log2_chroma_h);
+    s->planeheight[0] = s->planeheight[3] = inlink->h;
+    s->planewidth[1]  = s->planewidth[2]  = AV_CEIL_RSHIFT(inlink->w, desc->log2_chroma_w);
+    s->planewidth[0]  = s->planewidth[3]  = inlink->w;
+    sum = 0;
+    for (j = 0; j < s->nb_components; j++)
+        sum += s->planeheight[j] * s->planewidth[j];
+    average_max = 0;
+    for (j = 0; j < s->nb_components; j++) {
+        s->planeweight[j] = (double) s->planeheight[j] * s->planewidth[j] / sum;
+        average_max += s->max[j] * s->planeweight[j];
+    }
+    s->average_max = lrint(average_max);
+
+    s->dsp.sse_line = desc->comp[0].depth > 8 ? sse_line_16bit : sse_line_8bit;
+    if (ARCH_X86)
+        ff_psnr_init_x86(&s->dsp, desc->comp[0].depth);
+
+    return 0;
+}
+
+static int config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    ImgHashContext *s = ctx->priv;
+    AVFilterLink *mainlink = ctx->inputs[0];
+    int ret;
+
+    ret = ff_framesync_init_dualinput(&s->fs, ctx);
+    if (ret < 0)
+        return ret;
+    outlink->w = mainlink->w;
+    outlink->h = mainlink->h;
+    outlink->time_base = mainlink->time_base;
+    outlink->sample_aspect_ratio = mainlink->sample_aspect_ratio;
+    outlink->frame_rate = mainlink->frame_rate;
+    if ((ret = ff_framesync_configure(&s->fs)) < 0)
+        return ret;
+
+    return 0;
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    ImgHashContext *s = ctx->priv;
+    return ff_framesync_activate(&s->fs);
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    ImgHashContext *s = ctx->priv;
+
+    if (s->nb_frames > 0) {
+        int j;
+        char buf[256];
+
+        buf[0] = 0;
+        for (j = 0; j < s->nb_components; j++) {
+            int c = s->is_rgb ? s->rgba_map[j] : j;
+            av_strlcatf(buf, sizeof(buf), " %c:%f", s->comps[j],
+                        get_psnr(s->mse_comp[c], s->nb_frames, s->max[c]));
+        }
+        av_log(ctx, AV_LOG_WARNING, "PHQM average:%f PSNR%s average:%f min:%f max:%f\n",
+               s->hd / s->nb_frames,
+               buf,
+               get_psnr(s->mse, s->nb_frames, s->average_max),
+               get_psnr(s->max_mse, 1, s->average_max),
+               get_psnr(s->min_mse, 1, s->average_max));
+    }
+
+    ff_framesync_uninit(&s->fs);
+
+    if (s->stats_file && s->stats_file != stdout)
+        fclose(s->stats_file);
+    av_frame_free(&s->prev_picref);
+}
+
+static const AVFilterPad img_hash_inputs[] = {
+    {
+        .name         = "main",
+        .type         = AVMEDIA_TYPE_VIDEO,
+    },{
+        .name         = "reference",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_input_ref,
+    },
+    { NULL }
+};
+
+static const AVFilterPad img_hash_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_output,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_img_hash= {
+    .name          = "img_hash",
+    .description   = NULL_IF_CONFIG_SMALL("PHQM: Calculate the Perceptual Hash Hamming Difference between two video streams."),
+    .preinit       = img_hash_framesync_preinit,
+    .init          = init,
+    .uninit        = uninit,
+    .query_formats = query_formats,
+    .activate      = activate,
+    .priv_size     = sizeof(ImgHashContext),
+    .priv_class    = &img_hash_class,
+    .inputs        = img_hash_inputs,
+    .outputs       = img_hash_outputs,
+};
diff --git a/libavfilter/vf_perceptual.c b/libavfilter/vf_perceptual.c
new file mode 100644
index 0000000000..be67a48b7b
--- /dev/null
+++ b/libavfilter/vf_perceptual.c
@@ -0,0 +1,160 @@
+/*
+ * PERCEPTUAL FILTER: Chris Kennedy (C) 2019
+ *
+ * Perceptual scoring frame matching
+ *
+ * Tags decoded frames with a perceptual score for the x264
+ * encoder to use.
+ *
+ * License: Apache 2.0
+ *
+ */
+
+/**
+ * @file
+ * perceptual: Perceptual Scoring
+ */
+
+#include "libavutil/adler32.h"
+#include "libavutil/avstring.h"
+#include "libavutil/common.h"
+#include "libavutil/file.h"
+#include "libavutil/opt.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/timestamp.h"
+#include "libavfilter/lswsutils.h"
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+#include "img_hash.h"
+
+typedef struct {
+    const AVClass *class;
+    char *hash_type;
+    double score_multiplier;
+    double score_factor;
+    int hash_type_i;
+    uint64_t frame_count; // frame counter/number
+    AVFrame *lastframe;
+
+    void *priv; // private structure for filter, std FFmpeg API
+} PerceptualContext;
+
+// decides color space raw frame format used via standard LibAV API
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_YUV420P, AV_PIX_FMT_BGR24, AV_PIX_FMT_GRAY8, AV_PIX_FMT_NONE
+    };
+    return ff_set_common_formats(ctx, ff_make_format_list(pix_fmts));
+}
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    PerceptualContext *perceptual = ctx->priv;
+
+    if (!(perceptual->priv = av_mallocz(sizeof(PerceptualContext))))
+        return AVERROR(ENOMEM);
+
+    if (perceptual->hash_type) {
+        if (!strcmp(perceptual->hash_type, "phash"))
+            perceptual->hash_type_i = PHASH;
+        else if (!strcmp(perceptual->hash_type, "colormoment")) {
+            perceptual->hash_type_i = COLORMOMENT;
+            perceptual->score_multiplier = 1.0;
+        } else if (!strcmp(perceptual->hash_type, "average"))
+            perceptual->hash_type_i = AVERAGE;
+        else if (!strcmp(perceptual->hash_type, "marrhildreth"))
+            perceptual->hash_type_i = MARRHILDRETH;
+        else if (!strcmp(perceptual->hash_type, "radialvariance"))
+            perceptual->hash_type_i = RADIALVARIANCE;
+        else if (!strcmp(perceptual->hash_type, "blockmean1"))
+            perceptual->hash_type_i = BLOCKMEAN1;
+        else if (!strcmp(perceptual->hash_type, "blockmean2"))
+            perceptual->hash_type_i = BLOCKMEAN2;
+        else {
+            av_log(perceptual, AV_LOG_ERROR, "Bad hash_type given %s\n", perceptual->hash_type);
+            return AVERROR(ENOMEM);
+        }
+    }
+    return 0;
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    PerceptualContext *perceptual = ctx->priv;
+
+    av_free(perceptual->priv);
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    AVFilterContext *ctx = inlink->dst;
+    AVFilterLink *outlink= inlink->dst->outputs[0];
+    PerceptualContext *perceptual = ctx->priv;
+    double score = 0;
+
+    perceptual->frame_count++;
+
+    /* compare last frame with this current frame */
+    if (perceptual->frame_count > 1) {
+        score = getScore(perceptual->lastframe, in, inlink->format, perceptual->hash_type_i);
+        av_log(perceptual, AV_LOG_DEBUG, "Perceptual: %s hamming score %0.1lf\n", perceptual->hash_type, score);
+
+        /* mark frame, normalize to percentage */
+        in->perceptual_score = .01 * FFMIN((score * perceptual->score_multiplier), 100);
+        in->perceptual_score_factor = perceptual->score_factor;
+
+        av_frame_free(&perceptual->lastframe);
+    }
+    /* save last frame */
+    perceptual->lastframe = av_frame_clone(in);
+
+    av_log(perceptual, AV_LOG_DEBUG, "Perceptual: %%%0.1lf frame change\n", in->perceptual_score * 100);
+
+    return ff_filter_frame(outlink, in);
+}
+
+#define OFFSET(x) offsetof(PerceptualContext, x)
+#define FLAGS AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM
+static const AVOption perceptual_options[] = {
+    { "hash_type", "options: phash, colormoment, average", OFFSET(hash_type), AV_OPT_TYPE_STRING, {.str = "phash"}, .flags = FLAGS },
+    { "score_multiplier", "multiply the hamming score result by this value. 2.0 by default", OFFSET(score_multiplier),
+        AV_OPT_TYPE_DOUBLE, {.dbl = 2.0}, 0, 100, .flags = FLAGS },
+    { "score_factor", "factor to decrease compression, multiplier for bitrate, range for crf. 2.0 default", OFFSET(score_factor),
+        AV_OPT_TYPE_DOUBLE, {.dbl = 2.0}, 0, 1000, .flags = FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(perceptual);
+
+static const AVFilterPad avfilter_vf_perceptual_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad avfilter_vf_perceptual_outputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_perceptual = {
+    .name          = "perceptual",
+    .description   = NULL_IF_CONFIG_SMALL("Perceptual Scoring Filter"),
+    .priv_size     = sizeof(PerceptualContext),
+    .priv_class    = &perceptual_class,
+    .query_formats = query_formats,
+    .init          = init,
+    .uninit        = uninit,
+    .inputs        = avfilter_vf_perceptual_inputs,
+    .outputs       = avfilter_vf_perceptual_outputs,
+};
diff --git a/libavformat/mpegenc.c b/libavformat/mpegenc.c
index 43ebc46e0e..f6aa705e96 100644
--- a/libavformat/mpegenc.c
+++ b/libavformat/mpegenc.c
@@ -984,7 +984,7 @@ static int remove_decoded_packets(AVFormatContext *ctx, int64_t scr)
                scr > pkt_desc->dts) { // FIXME: > vs >=
             if (stream->buffer_index < pkt_desc->size ||
                 stream->predecode_packet == stream->premux_packet) {
-                av_log(ctx, AV_LOG_ERROR,
+                av_log(ctx, AV_LOG_WARNING,
                        "buffer underflow st=%d bufi=%d size=%d\n",
                        i, stream->buffer_index, pkt_desc->size);
                 break;
@@ -1063,7 +1063,7 @@ retry:
                     scr / 90000.0, best_dts / 90000.0);
 
             if (scr >= best_dts + 1 && !ignore_constraints) {
-                av_log(ctx, AV_LOG_ERROR,
+                av_log(ctx, AV_LOG_WARNING,
                     "packet too large, ignoring buffer limits to mux it\n");
                 ignore_constraints = 1;
             }
diff --git a/libavutil/frame.c b/libavutil/frame.c
index dcf1fc3d17..94e7644d1f 100644
--- a/libavutil/frame.c
+++ b/libavutil/frame.c
@@ -163,6 +163,8 @@ FF_ENABLE_DEPRECATION_WARNINGS
     frame->color_range         = AVCOL_RANGE_UNSPECIFIED;
     frame->chroma_location     = AVCHROMA_LOC_UNSPECIFIED;
     frame->flags               = 0;
+    frame->perceptual_score    = -1;
+    frame->perceptual_score_factor    = 2.0;
 }
 
 static void free_side_data(AVFrameSideData **ptr_sd)
@@ -373,6 +375,8 @@ FF_ENABLE_DEPRECATION_WARNINGS
     dst->colorspace             = src->colorspace;
     dst->color_range            = src->color_range;
     dst->chroma_location        = src->chroma_location;
+    dst->perceptual_score       = src->perceptual_score;
+    dst->perceptual_score_factor       = src->perceptual_score_factor;
 
     av_dict_copy(&dst->metadata, src->metadata, 0);
 
@@ -453,6 +457,8 @@ int av_frame_ref(AVFrame *dst, const AVFrame *src)
     dst->channels       = src->channels;
     dst->channel_layout = src->channel_layout;
     dst->nb_samples     = src->nb_samples;
+    dst->perceptual_score = src->perceptual_score;
+    dst->perceptual_score_factor = src->perceptual_score_factor;
 
     ret = frame_copy_props(dst, src, 0);
     if (ret < 0)
diff --git a/libavutil/frame.h b/libavutil/frame.h
index 5d3231e7bb..c9df011aaa 100644
--- a/libavutil/frame.h
+++ b/libavutil/frame.h
@@ -672,6 +672,13 @@ typedef struct AVFrame {
      * for the target frame's private_ref field.
      */
     AVBufferRef *private_ref;
+
+    /**
+     * perceptual score
+     * 0.00 - 1.00 percentage of perceptual match to the previous frame
+     */
+    float perceptual_score;
+    float perceptual_score_factor;
 } AVFrame;
 
 #if FF_API_FRAME_GET_SET
