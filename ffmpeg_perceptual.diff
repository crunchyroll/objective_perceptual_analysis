diff --git a/configure b/configure
index 34c2adb..cecc42d 100755
--- a/configure
+++ b/configure
@@ -254,6 +254,7 @@ External library support:
   --enable-libopenmpt      enable decoding tracked files via libopenmpt [no]
   --enable-libopus         enable Opus de/encoding via libopus [no]
   --enable-libpulse        enable Pulseaudio input via libpulse [no]
+  --enable-librav1e        enable AV1 encoding via rav1e [no]
   --enable-librsvg         enable SVG rasterization via librsvg [no]
   --enable-librubberband   enable rubberband needed for rubberband filter [no]
   --enable-librtmp         enable RTMP[E] support via librtmp [no]
@@ -1784,6 +1785,7 @@ EXTERNAL_LIBRARY_LIST="
     libopenmpt
     libopus
     libpulse
+    librav1e
     librsvg
     librtmp
     libshine
@@ -3185,6 +3187,7 @@ libopenmpt_demuxer_deps="libopenmpt"
 libopus_decoder_deps="libopus"
 libopus_encoder_deps="libopus"
 libopus_encoder_select="audio_frame_queue"
+librav1e_encoder_deps="librav1e"
 librsvg_decoder_deps="librsvg"
 libshine_encoder_deps="libshine"
 libshine_encoder_select="audio_frame_queue"
@@ -3487,6 +3490,10 @@ nlmeans_opencl_filter_deps="opencl"
 nnedi_filter_deps="gpl"
 ocr_filter_deps="libtesseract"
 ocv_filter_deps="libopencv"
+perceptual_filter_deps="libopencv"
+perceptual_filter_extralibs="-lstdc++ -lopencv_img_hash"
+img_hash_filter_deps="libopencv"
+img_hash_filter_extralibs="-lstdc++ -lopencv_img_hash"
 openclsrc_filter_deps="opencl"
 overlay_opencl_filter_deps="opencl"
 overlay_qsv_filter_deps="libmfx"
@@ -6252,6 +6259,7 @@ enabled libopus           && {
     }
 }
 enabled libpulse          && require_pkg_config libpulse libpulse pulse/pulseaudio.h pa_context_new
+enabled librav1e          && require_pkg_config librav1e rav1e rav1e.h rav1e_context_new
 enabled librsvg           && require_pkg_config librsvg librsvg-2.0 librsvg-2.0/librsvg/rsvg.h rsvg_handle_render_cairo
 enabled librtmp           && require_pkg_config librtmp librtmp librtmp/rtmp.h RTMP_Socket
 enabled librubberband     && require_pkg_config librubberband "rubberband >= 1.8.1" rubberband/rubberband-c.h rubberband_new -lstdc++ && append librubberband_extralibs "-lstdc++"
diff --git a/doc/encoders.texi b/doc/encoders.texi
index eefd124..2ed7053 100644
--- a/doc/encoders.texi
+++ b/doc/encoders.texi
@@ -1378,6 +1378,35 @@ makes it possible to store non-rgb pix_fmts.
 
 @end table
 
+@section librav1e
+
+rav1e AV1 encoder wrapper.
+
+Requires the presence of the rav1e headers and library from crav1e
+during configuration. You need to explicitly configue the build with
+@code{--enable-librav1e}.
+
+@subsection Options
+
+@table @option
+@item max-quantizer
+Sets the maximum qauntizer (floor) to use when using bitrate mode.
+
+@item quantizer
+Uses quantizers mode to encode at the given quantizer.
+
+@item rav1e-params
+Set rav1e options using a list of @var{key}=@var{value} couples separated
+by ":". See @command{rav1e --help} for a list of options.
+
+For example to specify librav1e encoding options with @option{-rav1e-params}:
+
+@example
+ffmpeg -i input -c:v librav1e -rav1e-params speed=5:low_latency=true output.mp4
+@end example
+
+@end table
+
 @section libaom-av1
 
 libaom AV1 encoder wrapper.
diff --git a/doc/general.texi b/doc/general.texi
index 3c0c803..50939f7 100644
--- a/doc/general.texi
+++ b/doc/general.texi
@@ -243,6 +243,13 @@ FFmpeg can use the OpenJPEG libraries for decoding/encoding J2K videos.  Go to
 instructions.  To enable using OpenJPEG in FFmpeg, pass @code{--enable-libopenjpeg} to
 @file{./configure}.
 
+@section rav1e
+
+FFmpeg can make use of rav1e (Rust AV1 Encoder) via its C bindings to encode videos.
+Go to @url{https://github.com/lu-zero/crav1e/} and @url{https://github.com/xiph/rav1e/}
+and follow the instructions. To enable using rav1e in FFmpeg, pass @code{--enable-librav1e}
+to @file{./configure}.
+
 @section TwoLAME
 
 FFmpeg can make use of the TwoLAME library for MP2 encoding.
diff --git a/libavcodec/Makefile b/libavcodec/Makefile
index 3cd73fb..cebc9d2 100644
--- a/libavcodec/Makefile
+++ b/libavcodec/Makefile
@@ -988,6 +988,7 @@ OBJS-$(CONFIG_LIBOPUS_DECODER)            += libopusdec.o libopus.o     \
                                              vorbis_data.o
 OBJS-$(CONFIG_LIBOPUS_ENCODER)            += libopusenc.o libopus.o     \
                                              vorbis_data.o
+OBJS-$(CONFIG_LIBRAV1E_ENCODER)           += librav1e.o
 OBJS-$(CONFIG_LIBSHINE_ENCODER)           += libshine.o
 OBJS-$(CONFIG_LIBSPEEX_DECODER)           += libspeexdec.o
 OBJS-$(CONFIG_LIBSPEEX_ENCODER)           += libspeexenc.o
diff --git a/libavcodec/allcodecs.c b/libavcodec/allcodecs.c
index d2f9a39..c1cc29e 100644
--- a/libavcodec/allcodecs.c
+++ b/libavcodec/allcodecs.c
@@ -703,6 +703,7 @@ extern AVCodec ff_libopenjpeg_encoder;
 extern AVCodec ff_libopenjpeg_decoder;
 extern AVCodec ff_libopus_encoder;
 extern AVCodec ff_libopus_decoder;
+extern AVCodec ff_librav1e_encoder;
 extern AVCodec ff_librsvg_decoder;
 extern AVCodec ff_libshine_encoder;
 extern AVCodec ff_libspeex_encoder;
diff --git a/libavcodec/librav1e.c b/libavcodec/librav1e.c
new file mode 100644
index 0000000..d1a9e54
--- /dev/null
+++ b/libavcodec/librav1e.c
@@ -0,0 +1,452 @@
+/*
+ * librav1e encoder
+ *
+ * Copyright (c) 2019 Derek Buitenhuis
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <rav1e.h>
+
+#include "libavutil/internal.h"
+#include "libavutil/common.h"
+#include "libavutil/opt.h"
+#include "libavutil/pixdesc.h"
+#include "avcodec.h"
+#include "internal.h"
+
+typedef struct PacketList {
+    RaPacket *pkt;
+    struct PacketList *next;
+} PacketList;
+
+typedef struct librav1eContext {
+    const AVClass *class;
+
+    RaContext *ctx;
+    AVBSFContext *bsf;
+    PacketList *pktlist;
+    char *rav1e_opts;
+    int max_quantizer;
+    int quantizer;
+    int done;
+} librav1eContext;
+
+static inline RaPixelRange range_map(enum AVColorRange range)
+
+{
+    switch (range) {
+    case AVCOL_RANGE_MPEG:
+        return RA_PIXEL_RANGE_LIMITED;
+    case AVCOL_RANGE_JPEG:
+        return RA_PIXEL_RANGE_FULL;
+    default:
+        return 0;
+    }
+}
+
+static inline RaChromaSampling pix_fmt_map(enum AVPixelFormat pix_fmt)
+{
+    switch (pix_fmt) {
+    case AV_PIX_FMT_YUV420P:
+    case AV_PIX_FMT_YUV420P10:
+    case AV_PIX_FMT_YUV420P12:
+        return RA_CHROMA_SAMPLING_CS420;
+    case AV_PIX_FMT_YUV422P:
+    case AV_PIX_FMT_YUV422P10:
+    case AV_PIX_FMT_YUV422P12:
+        return RA_CHROMA_SAMPLING_CS422;
+    case AV_PIX_FMT_YUV444P:
+    case AV_PIX_FMT_YUV444P10:
+    case AV_PIX_FMT_YUV444P12:
+        return RA_CHROMA_SAMPLING_CS444;
+    case AV_PIX_FMT_GRAY8:
+    case AV_PIX_FMT_GRAY10:
+    case AV_PIX_FMT_GRAY12:
+        return RA_CHROMA_SAMPLING_CS400;
+    default:
+        // This should be impossible
+        return (RaChromaSampling) -1;
+    }
+}
+
+static inline RaChromaSamplePosition chroma_loc_map(enum AVChromaLocation chroma_loc)
+{
+    switch (chroma_loc) {
+    case AVCHROMA_LOC_LEFT:
+        return RA_CHROMA_SAMPLE_POSITION_VERTICAL;
+    case AVCHROMA_LOC_TOPLEFT:
+        return RA_CHROMA_SAMPLE_POSITION_COLOCATED;
+    default:
+        return RA_CHROMA_SAMPLE_POSITION_UNKNOWN;
+    }
+}
+
+static int add_packet(PacketList **list, RaPacket *pkt)
+{
+    PacketList *cur = *list;
+    PacketList *newentry = av_mallocz(sizeof(PacketList));
+    if (!newentry)
+        return AVERROR(ENOMEM);
+
+    newentry->pkt = pkt;
+
+    if (!cur) {
+        *list = newentry;
+        return 0;
+    }
+
+    /*
+     * Just use a simple linear search, since the reoroder buffer in
+     * AV1 is capped to something fairly low.
+     */
+    while (cur->next)
+        cur = cur->next;
+
+    cur->next = newentry;
+
+    return 0;
+}
+
+static RaPacket *get_packet(PacketList **list)
+{
+    PacketList *head = *list;
+    RaPacket *ret;
+
+    if (!head)
+        return NULL;
+
+    ret = head->pkt;
+
+    *list = head->next;
+    av_free(head);
+
+    return ret;
+}
+
+static av_cold int librav1e_encode_close(AVCodecContext *avctx)
+{
+    librav1eContext *ctx = avctx->priv_data;
+
+    if (ctx->ctx) {
+        rav1e_context_unref(ctx->ctx);
+        ctx->ctx = NULL;
+    }
+
+    while (ctx->pktlist) {
+        RaPacket *rpkt = get_packet(&ctx->pktlist);
+        rav1e_packet_unref(rpkt);
+    }
+
+    av_bsf_free(&ctx->bsf);
+
+    return 0;
+}
+
+static av_cold int librav1e_encode_init(AVCodecContext *avctx)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(avctx->pix_fmt);
+    RaConfig *cfg = NULL;
+    int rret;
+    int ret = 0;
+
+    cfg = rav1e_config_default();
+    if (!cfg) {
+        av_log(avctx, AV_LOG_ERROR, "Could not allocate rav1e config.\n");
+        ret = AVERROR_EXTERNAL;
+        goto end;
+    }
+
+    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {
+         const AVBitStreamFilter *filter = av_bsf_get_by_name("extract_extradata");
+         int bret;
+
+         if (!filter) {
+            av_log(avctx, AV_LOG_ERROR, "extract_extradata bitstream filter "
+                   "not found. This is a bug, please report it.\n");
+            ret = AVERROR_BUG;
+            goto end;
+         }
+
+         bret = av_bsf_alloc(filter, &ctx->bsf);
+         if (bret < 0) {
+             ret = bret;
+             goto end;
+         }
+
+         bret = avcodec_parameters_from_context(ctx->bsf->par_in, avctx);
+         if (bret < 0) {
+             ret = bret;
+             goto end;
+         }
+
+         bret = av_bsf_init(ctx->bsf);
+         if (bret < 0) {
+             ret = bret;
+             goto end;
+         }
+    }
+
+    if (ctx->rav1e_opts) {
+        AVDictionary *dict    = NULL;
+        AVDictionaryEntry *en = NULL;
+
+        if (!av_dict_parse_string(&dict, ctx->rav1e_opts, "=", ":", 0)) {
+            while (en = av_dict_get(dict, "", en, AV_DICT_IGNORE_SUFFIX)) {
+                int parse_ret = rav1e_config_parse(cfg, en->key, en->value);
+                if (parse_ret < 0)
+                    av_log(avctx, AV_LOG_WARNING, "Invalid value for %s: %s.\n", en->key, en->value);
+            }
+            av_dict_free(&dict);
+        }
+    }
+
+    rret = rav1e_config_parse_int(cfg, "width", avctx->width);
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Invalid width passed to rav1e.\n");
+        ret = AVERROR_INVALIDDATA;
+        goto end;
+    }
+
+    rret = rav1e_config_parse_int(cfg, "height", avctx->height);
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Invalid width passed to rav1e.\n");
+        ret = AVERROR_INVALIDDATA;
+        goto end;
+    }
+
+    rret = rav1e_config_parse_int(cfg, "threads", avctx->thread_count);
+    if (rret < 0)
+        av_log(avctx, AV_LOG_WARNING, "Invalid number of threads, defaulting to auto.\n");
+
+    if (avctx->bit_rate && ctx->quantizer < 0) {
+        rret = rav1e_config_parse_int(cfg, "quantizer", ctx->max_quantizer);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set max quantizer.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+        rret = rav1e_config_parse_int(cfg, "bitrate", avctx->bit_rate);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set bitrate.\n");
+            ret = AVERROR_INVALIDDATA;
+            goto end;
+        }
+    } else if (ctx->quantizer >= 0) {
+        rret = rav1e_config_parse_int(cfg, "quantizer", ctx->quantizer);
+        if (rret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not set quantizer.\n");
+            ret = AVERROR_EXTERNAL;
+            goto end;
+        }
+    }
+
+    rret = rav1e_config_set_pixel_format(cfg, desc->comp[0].depth,
+                                         pix_fmt_map(avctx->pix_fmt),
+                                         chroma_loc_map(avctx->chroma_sample_location),
+                                         range_map(avctx->color_range));
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Failed to set pixel format properties.\n");
+        ret = AVERROR_INVALIDDATA;
+        goto end;
+    }
+
+    /* rav1e's colorspace enums match standard values. */
+    rret = rav1e_config_set_color_description(cfg, (RaMatrixCoefficients) avctx->colorspace,
+                                              (RaColorPrimaries) avctx->color_primaries,
+                                              (RaTransferCharacteristics) avctx->color_trc);
+    if (rret < 0) {
+        av_log(avctx, AV_LOG_WARNING, "Failed to set color properties.\n");
+        if (avctx->err_recognition & AV_EF_EXPLODE) {
+            ret = AVERROR_INVALIDDATA;
+            goto end;
+        }
+    }
+
+    ctx->ctx = rav1e_context_new(cfg);
+    if (!ctx->ctx) {
+        av_log(avctx, AV_LOG_ERROR, "Failed to create rav1e encode context.\n");
+        ret = AVERROR_EXTERNAL;
+        goto end;
+    }
+
+    ret = 0;
+
+end:
+    if (cfg)
+        rav1e_config_unref(cfg);
+
+    if (ret)
+        librav1e_encode_close(avctx);
+
+    return ret;
+}
+
+static int librav1e_encode_frame(AVCodecContext *avctx, AVPacket *pkt,
+                                 const AVFrame *pic, int *got_packet)
+{
+    librav1eContext *ctx = avctx->priv_data;
+    RaPacket *rpkt = NULL;
+    RaFrame *rframe = NULL;
+    RaEncoderStatus ret;
+    int pret;
+
+    if (pic) {
+        const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(pic->format);
+
+        rframe = rav1e_frame_new(ctx->ctx);
+        if (!rframe) {
+            av_log(avctx, AV_LOG_ERROR, "Could not allocate new rav1e frame.\n");
+            return AVERROR(ENOMEM);
+        }
+
+        for (int i = 0; i < 3; i++) {
+            int shift = i ? desc->log2_chroma_h : 0;
+            int bytes = desc->comp[0].depth == 8 ? 1 : 2;
+            rav1e_frame_fill_plane(rframe, i, pic->data[i],
+                                   (pic->height >> shift) * pic->linesize[i],
+                                   pic->linesize[i], bytes);
+        }
+    }
+
+    ret = rav1e_send_frame(ctx->ctx, rframe);
+    if (rframe)
+         rav1e_frame_unref(rframe); /* No need to unref if flushing. */
+    if (ret < 0) {
+        av_log(avctx, AV_LOG_ERROR, "Could not send frame.\n");
+        return AVERROR_EXTERNAL;
+    } else if (ret == RA_ENCODER_STATUS_ENOUGH_DATA) {
+        av_log(avctx, AV_LOG_WARNING, "rav1e encode queue is full. Frames may be dropped.\n");
+    }
+
+    while (!ctx->done) {
+        ret = rav1e_receive_packet(ctx->ctx, &rpkt);
+        if (ret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "Could not encode frame.\n");
+            return AVERROR_EXTERNAL;
+        } else if (ret == RA_ENCODER_STATUS_NEED_MORE_DATA || ret == RA_ENCODER_STATUS_ENCODED) {
+            break;
+        } else if (ret == RA_ENCODER_STATUS_LIMIT_REACHED) {
+            /* We're done. Nothing else to flush, so stop tryng. */
+            ctx->done = 1;
+            break;
+        } else if (ret == RA_ENCODER_STATUS_SUCCESS) {
+            /*
+             * Since we must drain the encoder of packets when we finally successfully
+             * receive one, add it to a packet queue and output as need be. Since this
+             * is only due to the frame "reordering" (alt-ref) internal to the encoder,
+             * it should never get very big before all being output. This is is similar
+             * to what is done in libaomenc.c.
+             */
+            int aret = add_packet(&ctx->pktlist, rpkt);
+            if (aret < 0) {
+                rav1e_packet_unref(rpkt);
+                return aret;
+            }
+            rpkt = NULL;
+        } else {
+            av_log(avctx, AV_LOG_ERROR, "Unknown return code from ra1ve_receive_packet.\n");
+            return AVERROR_UNKNOWN;
+        }
+    }
+
+    rpkt = get_packet(&ctx->pktlist);
+    if (!rpkt)
+        return 0;
+
+    pret = ff_alloc_packet2(avctx, pkt, rpkt->len, rpkt->len);
+    if (pret < 0) {
+        rav1e_packet_unref(rpkt);
+        av_log(avctx, AV_LOG_ERROR, "Error getting output packet.\n");
+        return ret;
+    }
+
+    memcpy(pkt->data, rpkt->data, rpkt->len);
+
+    if (rpkt->frame_type == RA_FRAME_TYPE_KEY)
+        pkt->flags |= AV_PKT_FLAG_KEY;
+
+    //pkt->pts = pkt->dts = rpkt->number * avctx->ticks_per_frame;
+    pkt->pts = pkt->dts = avctx->time_base.num * avctx->ticks_per_frame;
+
+    rav1e_packet_unref(rpkt);
+
+    if (avctx->flags & AV_CODEC_FLAG_GLOBAL_HEADER) {
+        pret = av_bsf_send_packet(ctx->bsf, pkt);
+        if (pret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "extradata extraction send failed.\n");
+            return pret;
+        }
+
+        pret = av_bsf_receive_packet(ctx->bsf, pkt);
+        if (pret < 0) {
+            av_log(avctx, AV_LOG_ERROR, "extradata extraction receive failed.\n");
+            return pret;
+        }
+    }
+
+    *got_packet = 1;
+
+    return 0;
+}
+
+#define OFFSET(x) offsetof(librav1eContext, x)
+#define VE AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_ENCODING_PARAM
+
+static const AVOption options[] = {
+    { "quantizer", "use constant quantizer mode", OFFSET(quantizer), AV_OPT_TYPE_INT, { .i64 = -1 }, -1, 255, VE },
+    { "max-quantizer", "max quantizer when using bitrate mode", OFFSET(max_quantizer), AV_OPT_TYPE_INT, { .i64 = 255 }, 1, 255, VE },
+    { "rav1e-params", "set the rav1e configuration using a :-separated list of key=value parameters", OFFSET(rav1e_opts), AV_OPT_TYPE_STRING, { 0 }, 0, 0, VE },
+    { NULL }
+};
+
+static const AVClass class = {
+    .class_name = "librav1e",
+    .item_name  = av_default_item_name,
+    .option     = options,
+    .version    = LIBAVUTIL_VERSION_INT,
+};
+
+AVCodec ff_librav1e_encoder = {
+    .name           = "librav1e",
+    .long_name      = NULL_IF_CONFIG_SMALL("librav1e AV1"),
+    .type           = AVMEDIA_TYPE_VIDEO,
+    .id             = AV_CODEC_ID_AV1,
+    .init           = librav1e_encode_init,
+    .encode2        = librav1e_encode_frame,
+    .close          = librav1e_encode_close,
+    .priv_data_size = sizeof(librav1eContext),
+    .priv_class     = &class,
+    .pix_fmts       = (const enum AVPixelFormat[]) {
+        AV_PIX_FMT_YUV420P,
+        AV_PIX_FMT_YUV420P10,
+        AV_PIX_FMT_YUV420P12,
+        AV_PIX_FMT_YUV422P,
+        AV_PIX_FMT_YUV422P10,
+        AV_PIX_FMT_YUV422P12,
+        AV_PIX_FMT_YUV444P,
+        AV_PIX_FMT_YUV444P10,
+        AV_PIX_FMT_YUV444P12,
+        AV_PIX_FMT_GRAY8,
+        AV_PIX_FMT_GRAY10,
+        AV_PIX_FMT_GRAY12,
+        AV_PIX_FMT_NONE
+    },
+    .capabilities   = AV_CODEC_CAP_DELAY | AV_CODEC_CAP_AUTO_THREADS,
+    .wrapper_name   = "librav1e",
+};
diff --git a/libavcodec/libx264.c b/libavcodec/libx264.c
index dc4b4b1..cedcdef 100644
--- a/libavcodec/libx264.c
+++ b/libavcodec/libx264.c
@@ -195,24 +195,73 @@ static void reconfig_encoder(AVCodecContext *ctx, const AVFrame *frame)
         x264_encoder_reconfig(x4->enc, &x4->params);
     }
 
-    if (x4->params.rc.i_vbv_buffer_size != ctx->rc_buffer_size / 1000 ||
-        x4->params.rc.i_vbv_max_bitrate != ctx->rc_max_rate    / 1000) {
-        x4->params.rc.i_vbv_buffer_size = ctx->rc_buffer_size / 1000;
-        x4->params.rc.i_vbv_max_bitrate = ctx->rc_max_rate    / 1000;
-        x264_encoder_reconfig(x4->enc, &x4->params);
+    if (frame->perceptual_score == -1) {
+        if (x4->params.rc.i_vbv_buffer_size != ctx->rc_buffer_size / 1000 ||
+            x4->params.rc.i_vbv_max_bitrate != ctx->rc_max_rate    / 1000) {
+            x4->params.rc.i_vbv_buffer_size = ctx->rc_buffer_size / 1000;
+            x4->params.rc.i_vbv_max_bitrate = ctx->rc_max_rate    / 1000;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        }
     }
 
     if (x4->params.rc.i_rc_method == X264_RC_ABR &&
-        x4->params.rc.i_bitrate != ctx->bit_rate / 1000) {
-        x4->params.rc.i_bitrate = ctx->bit_rate / 1000;
-        x264_encoder_reconfig(x4->enc, &x4->params);
+        (frame->perceptual_score > -1 ||
+         x4->params.rc.i_bitrate != ctx->bit_rate / 1000)) {
+        if (frame->perceptual_score > -1) {
+            int bitrate = 0;
+            /* set ABR bitrate value from perceptual score */
+            /* decrease compression by raising the avg bitrate up to N times */
+            bitrate = (ctx->bit_rate / 1000) + ((frame->perceptual_score * frame->perceptual_score_factor) * (ctx->bit_rate / 1000.0));
+            x4->params.rc.i_bitrate = bitrate;
+            x4->params.rc.i_vbv_max_bitrate = bitrate * 1.5;
+            x4->params.rc.i_vbv_buffer_size = bitrate * 1.5 * 1.5;
+            av_log(ctx, AV_LOG_DEBUG,
+               "Perceptual: bitrate %d maxbitrate %d from %"PRIu64"\n", 
+               x4->params.rc.i_bitrate, 
+               x4->params.rc.i_vbv_max_bitrate,
+               ctx->bit_rate / 1000);
+
+            /* tag this frame with this specific config */
+            x4->pic.param = &x4->params;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        } else {
+            x4->params.rc.i_bitrate = ctx->bit_rate / 1000;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        }
     }
 
     if (x4->crf >= 0 &&
         x4->params.rc.i_rc_method == X264_RC_CRF &&
-        x4->params.rc.f_rf_constant != x4->crf) {
-        x4->params.rc.f_rf_constant = x4->crf;
-        x264_encoder_reconfig(x4->enc, &x4->params);
+        (frame->perceptual_score > -1 ||
+         x4->params.rc.f_rf_constant != x4->crf)) {
+        if (frame->perceptual_score > -1) {
+            float crf_value = 0.0;
+            if (ctx->rc_max_rate) {
+                int bitrate = 0;
+                /* set crf value from perceptual score */
+                /* decrease compression by lowering the score by up to N CRF points */
+                crf_value = x4->crf - ((frame->perceptual_score * 100.0) / (frame->perceptual_score_factor * 2));
+                x4->params.rc.f_rf_constant = crf_value;
+
+                /* set ABR bitrate value from perceptual score */
+                /* decrease compression by raising the avg bitrate up to N times */
+                bitrate = (ctx->rc_max_rate / 1000) + ((frame->perceptual_score * frame->perceptual_score_factor) * (ctx->rc_max_rate / 1000.0));
+                x4->params.rc.i_vbv_max_bitrate = bitrate;
+                x4->params.rc.i_vbv_buffer_size = bitrate * 1.5 * 1.5;
+            }
+            av_log(ctx, AV_LOG_DEBUG,
+               "Perceptual: crf: %0.2f bitrate %d maxbitrate %d from %"PRIu64"\n", 
+               x4->params.rc.f_rf_constant,
+               x4->params.rc.i_bitrate, 
+               x4->params.rc.i_vbv_max_bitrate,
+               ctx->rc_max_rate / 1000);
+
+            /* tag this frame with this specific config */
+            x4->pic.param = &x4->params;
+        } else {
+            x4->params.rc.f_rf_constant = x4->crf;
+            x264_encoder_reconfig(x4->enc, &x4->params);
+        }
     }
 
     if (x4->params.rc.i_rc_method == X264_RC_CQP &&
diff --git a/libavfilter/Makefile b/libavfilter/Makefile
index 455c809..a51087f 100644
--- a/libavfilter/Makefile
+++ b/libavfilter/Makefile
@@ -307,6 +307,8 @@ OBJS-$(CONFIG_NORMALIZE_FILTER)              += vf_normalize.o
 OBJS-$(CONFIG_NULL_FILTER)                   += vf_null.o
 OBJS-$(CONFIG_OCR_FILTER)                    += vf_ocr.o
 OBJS-$(CONFIG_OCV_FILTER)                    += vf_libopencv.o
+OBJS-$(CONFIG_PERCEPTUAL_FILTER)             += vf_perceptual.o img_hash.o
+OBJS-$(CONFIG_IMG_HASH_FILTER)               += vf_img_hash.o img_hash.o
 OBJS-$(CONFIG_OSCILLOSCOPE_FILTER)           += vf_datascope.o
 OBJS-$(CONFIG_OVERLAY_FILTER)                += vf_overlay.o framesync.o
 OBJS-$(CONFIG_OVERLAY_OPENCL_FILTER)         += vf_overlay_opencl.o opencl.o \
@@ -489,6 +491,7 @@ OBJS-$(CONFIG_SHARED)                        += log2_tab.o
 SKIPHEADERS-$(CONFIG_QSVVPP)                 += qsvvpp.h
 SKIPHEADERS-$(CONFIG_OPENCL)                 += opencl.h
 SKIPHEADERS-$(CONFIG_VAAPI)                  += vaapi_vpp.h
+SKIPHEADERS-$(CONFIG_LIBOPENCV)              += img_hash.h
 
 TOOLS     = graph2dot
 TESTPROGS = drawutils filtfmts formats integral
diff --git a/libavfilter/allfilters.c b/libavfilter/allfilters.c
index 04a3df7..ab56563 100644
--- a/libavfilter/allfilters.c
+++ b/libavfilter/allfilters.c
@@ -292,6 +292,8 @@ extern AVFilter ff_vf_normalize;
 extern AVFilter ff_vf_null;
 extern AVFilter ff_vf_ocr;
 extern AVFilter ff_vf_ocv;
+extern AVFilter ff_vf_perceptual;
+extern AVFilter ff_vf_img_hash;
 extern AVFilter ff_vf_oscilloscope;
 extern AVFilter ff_vf_overlay;
 extern AVFilter ff_vf_overlay_opencl;
diff --git a/libavfilter/img_hash.cpp b/libavfilter/img_hash.cpp
new file mode 100644
index 0000000..e6bf32f
--- /dev/null
+++ b/libavfilter/img_hash.cpp
@@ -0,0 +1,86 @@
+// Christopher Kennedy 2019
+// License: Apache 2.0
+//
+// Originally inspired by the information from:
+// https://qtandopencv.blogspot.com/2016/06/introduction-to-image-hash-module-of.html
+//
+#include <opencv2/core.hpp>
+#include <opencv2/core/ocl.hpp>
+#include <opencv2/highgui.hpp>
+#include <opencv2/img_hash.hpp>
+#include <opencv2/imgproc.hpp>
+
+#include <iostream>
+
+#include "img_hash.h"
+#include "libavutil/pixdesc.h"
+extern "C" {
+#include "avfilter.h"
+}
+
+using namespace cv;
+using namespace cv::img_hash;
+using namespace std;
+
+// From the libopencv AVFilter
+static void fill_iplimage_from_frame(IplImage *img, const AVFrame *frame, enum AVPixelFormat pixfmt)
+{
+    IplImage *tmpimg;
+    int depth, channels_nb;
+
+    if      (pixfmt == AV_PIX_FMT_GRAY8) { depth = IPL_DEPTH_8U;  channels_nb = 1; }
+    else if (pixfmt == AV_PIX_FMT_BGRA)  { depth = IPL_DEPTH_8U;  channels_nb = 4; }
+    else if (pixfmt == AV_PIX_FMT_BGR24) { depth = IPL_DEPTH_8U;  channels_nb = 3; }
+    else if (pixfmt == AV_PIX_FMT_YUV420P) { depth = IPL_DEPTH_8U;  channels_nb = 3; }
+    else return;
+
+    tmpimg = cvCreateImageHeader((CvSize){frame->width, frame->height}, depth, channels_nb);
+    *img = *tmpimg;
+    img->imageData = img->imageDataOrigin = (char *) frame->data[0];
+    img->dataOrder = IPL_DATA_ORDER_PIXEL;
+    img->origin    = IPL_ORIGIN_TL;
+    img->widthStep = frame->linesize[0];
+}
+
+// Get the score of two Video Frames by comparing the perceptual hashes and deriving a hamming distance
+// showing how similar they are or different. lower the score is better for most algorithms
+extern "C" double getScore(const AVFrame *frame1, const AVFrame *frame2, enum AVPixelFormat pixfmt, int hash_type) {
+    cv::Ptr<cv::img_hash::ImgHashBase> algo;
+    IplImage ipl1, ipl2;
+    cv::Mat h1;
+    cv::Mat h2;
+    cv::Mat m1;
+    cv::Mat m2;
+
+    // Take FFmpeg video frame and convert into an IplImage for OpenCV
+    fill_iplimage_from_frame(&ipl1, frame1, pixfmt);
+    fill_iplimage_from_frame(&ipl2, frame2, pixfmt);
+    // Convert an IplImage to an Mat Image for OpenCV (newer format)
+    m1 = cv::cvarrToMat(&ipl1);
+    m2 = cv::cvarrToMat(&ipl2);
+
+    // substantiate the hash type algorithm
+    if (hash_type == COLORMOMENT) {
+        algo = cv::img_hash::ColorMomentHash::create();
+    } else if (hash_type == AVERAGE) {
+        algo = cv::img_hash::AverageHash::create();
+    } else if (hash_type == BLOCKMEAN1) {
+        //BlockMeanHash support mode 0 and mode 1, they associate to 
+        //    //mode 1 and mode 2 of PHash library
+        algo = cv::img_hash::BlockMeanHash::create(0);
+    } else if (hash_type == BLOCKMEAN2) {
+        algo = cv::img_hash::BlockMeanHash::create(1);
+    } else if (hash_type == MARRHILDRETH) {
+        algo = cv::img_hash::MarrHildrethHash::create();
+    } else if (hash_type == RADIALVARIANCE) {
+        algo = cv::img_hash::RadialVarianceHash::create();
+    } else { // Default to PHash
+        algo = cv::img_hash::PHash::create();
+    }
+    // Compute the hash
+    algo->compute(m1, h1);
+    algo->compute(m2, h2);
+    // Compare the hashes and return the hamming distance
+    return algo->compare(h1, h2);
+}
+
diff --git a/libavfilter/img_hash.h b/libavfilter/img_hash.h
new file mode 100644
index 0000000..c916766
--- /dev/null
+++ b/libavfilter/img_hash.h
@@ -0,0 +1,31 @@
+// Christopher Kennedy 2019
+// License: Apache 2.0
+//
+// Tutorial and code from
+// https://qtandopencv.blogspot.com/2016/06/introduction-to-image-hash-module-of.html
+//
+#ifndef AVFILTER_IMG_HASH_H
+#define AVFILTER_IMG_HASH_H
+
+#include "avfilter.h"
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+#define PHASH 0
+#define COLORMOMENT 1
+#define AVERAGE 2
+#define MARRHILDRETH 3
+#define RADIALVARIANCE 4
+#define BLOCKMEAN1 5
+#define BLOCKMEAN2 6
+
+double getScore(const AVFrame *frame1, const AVFrame *frame2, enum AVPixelFormat pixfmt, int hash_type);
+#ifdef __cplusplus
+}
+#endif
+
+
+#endif
diff --git a/libavfilter/vf_img_hash.c b/libavfilter/vf_img_hash.c
new file mode 100644
index 0000000..0959b3b
--- /dev/null
+++ b/libavfilter/vf_img_hash.c
@@ -0,0 +1,528 @@
+/*
+ * Copyright (c) 2019 Christopher Kennedy
+ *
+ * PHQM Perceptual Hash Quality Metric
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * PHQM: Caculate the Image Hash Hamming Difference between two input videos.
+ */
+
+#include "libavutil/avstring.h"
+#include "libavutil/opt.h"
+#include "libavutil/pixdesc.h"
+#include "avfilter.h"
+#include "drawutils.h"
+#include "formats.h"
+#include "framesync.h"
+#include "internal.h"
+#include "psnr.h"
+#include "video.h"
+
+#include "img_hash.h"
+#include "scene_sad.h"
+
+typedef struct ImgHashContext {
+    const AVClass *class;
+    FFFrameSync fs;
+    double shd, hd, min_hd, max_hd, smin_hd, smax_hd, mse, min_mse, max_mse, mse_comp[4];
+    uint64_t nb_shd;
+    uint64_t nb_frames;
+    FILE *stats_file;
+    char *stats_file_str;
+    int stats_version;
+    char *hash_type;
+    int hash_type_i;
+    int stats_header_written;
+    int stats_add_max;
+    int max[4], average_max;
+    int is_rgb;
+    uint8_t rgba_map[4];
+    char comps[4];
+    int nb_components;
+    int planewidth[4];
+    int planeheight[4];
+    double planeweight[4];
+    PSNRDSPContext dsp;
+    ff_scene_sad_fn sad;            ///< Sum of the absolute difference function (scene detect only)
+    double prev_mafd;               ///< previous MAFD                           (scene detect only)
+    AVFrame *prev_picref;           ///< previous frame                          (scene detect only)
+    double scd_thresh;
+    double scene_score;
+} ImgHashContext;
+
+#define OFFSET(x) offsetof(ImgHashContext, x)
+#define FLAGS AV_OPT_FLAG_FILTERING_PARAM|AV_OPT_FLAG_VIDEO_PARAM
+
+static const AVOption img_hash_options[] = {
+    {"stats_file", "Set file where to store per-frame difference information", OFFSET(stats_file_str), AV_OPT_TYPE_STRING, {.str=NULL}, 0, 0, FLAGS },
+    {"f",          "Set file where to store per-frame difference information", OFFSET(stats_file_str), AV_OPT_TYPE_STRING, {.str=NULL}, 0, 0, FLAGS },
+    {"stats_version", "Set the format version for the stats file.",               OFFSET(stats_version),  AV_OPT_TYPE_INT,    {.i64=1},    1, 2, FLAGS },
+    {"scd_thresh", "Scene Change Detection Threshold. 0.5 default, 0.0-1.0",    OFFSET(scd_thresh),  AV_OPT_TYPE_DOUBLE,    {.dbl=0.5},    0, 1, FLAGS },
+    {"output_max",  "Add raw stats (max values) to the output log.",            OFFSET(stats_add_max), AV_OPT_TYPE_BOOL, {.i64=0}, 0, 1, FLAGS},
+    { "hash_type", "options: phash, colormoment, average", OFFSET(hash_type), AV_OPT_TYPE_STRING, {.str = "phash"}, .flags = FLAGS },
+    { NULL }
+};
+
+FRAMESYNC_DEFINE_CLASS(img_hash, ImgHashContext, fs);
+
+static inline unsigned pow_2(unsigned base)
+{
+    return base*base;
+}
+
+static inline double get_psnr(double mse, uint64_t nb_frames, int max)
+{
+    return 10.0 * log10(pow_2(max) / (mse / nb_frames));
+}
+
+static uint64_t sse_line_8bit(const uint8_t *main_line,  const uint8_t *ref_line, int outw)
+{
+    int j;
+    unsigned m2 = 0;
+
+    for (j = 0; j < outw; j++)
+        m2 += pow_2(main_line[j] - ref_line[j]);
+
+    return m2;
+}
+
+static uint64_t sse_line_16bit(const uint8_t *_main_line, const uint8_t *_ref_line, int outw)
+{
+    int j;
+    uint64_t m2 = 0;
+    const uint16_t *main_line = (const uint16_t *) _main_line;
+    const uint16_t *ref_line = (const uint16_t *) _ref_line;
+
+    for (j = 0; j < outw; j++)
+        m2 += pow_2(main_line[j] - ref_line[j]);
+
+    return m2;
+}
+
+static inline
+void compute_images_mse(ImgHashContext *s,
+                        const uint8_t *main_data[4], const int main_linesizes[4],
+                        const uint8_t *ref_data[4], const int ref_linesizes[4],
+                        int w, int h, double mse[4])
+{
+    int i, c;
+
+    for (c = 0; c < s->nb_components; c++) {
+        const int outw = s->planewidth[c];
+        const int outh = s->planeheight[c];
+        const uint8_t *main_line = main_data[c];
+        const uint8_t *ref_line = ref_data[c];
+        const int ref_linesize = ref_linesizes[c];
+        const int main_linesize = main_linesizes[c];
+        uint64_t m = 0;
+        for (i = 0; i < outh; i++) {
+            m += s->dsp.sse_line(main_line, ref_line, outw);
+            ref_line += ref_linesize;
+            main_line += main_linesize;
+        }
+        mse[c] = m / (double)(outw * outh);
+    }
+}
+
+static void set_meta(AVDictionary **metadata, const char *key, char comp, float d)
+{
+    char value[128];
+    snprintf(value, sizeof(value), "%0.2f", d);
+    if (comp) {
+        char key2[128];
+        snprintf(key2, sizeof(key2), "%s%c", key, comp);
+        av_dict_set(metadata, key2, value, 0);
+    } else {
+        av_dict_set(metadata, key, value, 0);
+    }
+}
+
+static double get_scene_score(AVFilterContext *ctx, AVFrame *frame)
+{
+    double ret = 0;
+    ImgHashContext *s = ctx->priv;
+    AVFrame *prev_picref = s->prev_picref;
+
+    if (prev_picref &&
+        frame->height == prev_picref->height &&
+        frame->width  == prev_picref->width) {
+        uint64_t sad;
+        double mafd, diff;
+
+        s->sad(prev_picref->data[0], prev_picref->linesize[0], frame->data[0], frame->linesize[0], frame->width * 3, frame->height, &sad);
+        emms_c();
+        mafd = (double)sad / (frame->width * 3 * frame->height);
+        diff = fabs(mafd - s->prev_mafd);
+        ret  = av_clipf(FFMIN(mafd, diff) / 100., 0, 1);
+        s->prev_mafd = mafd;
+        av_frame_free(&prev_picref);
+    }
+    s->prev_picref = av_frame_clone(frame);
+    return ret;
+}
+
+static int do_phqm(FFFrameSync *fs)
+{
+    AVFilterContext *ctx = fs->parent;
+    ImgHashContext *s = ctx->priv;
+    AVFrame *master, *ref;
+    double comp_mse[4], mse = 0, hd = 0;
+    int ret, j, c;
+    double hd_limit = 1000000;
+    AVDictionary **metadata;
+
+    ret = ff_framesync_dualinput_get(fs, &master, &ref);
+    if (ret < 0)
+        return ret;
+    if (!ref)
+        return ff_filter_frame(ctx->outputs[0], master);
+    metadata = &master->metadata;
+
+    compute_images_mse(s, (const uint8_t **)master->data, master->linesize,
+                          (const uint8_t **)ref->data, ref->linesize,
+                          master->width, master->height, comp_mse);
+
+    for (j = 0; j < s->nb_components; j++)
+        mse += comp_mse[j] * s->planeweight[j];
+
+    s->min_mse = FFMIN(s->min_mse, mse);
+    s->max_mse = FFMAX(s->max_mse, mse);
+
+    s->mse += mse;
+    for (j = 0; j < s->nb_components; j++)
+        s->mse_comp[j] += comp_mse[j];
+    s->nb_frames++;
+
+    for (j = 0; j < s->nb_components; j++) {
+        c = s->is_rgb ? s->rgba_map[j] : j;
+        set_meta(metadata, "lavfi.psnr.mse.", s->comps[j], comp_mse[c]);
+        set_meta(metadata, "lavfi.psnr.psnr.", s->comps[j], get_psnr(comp_mse[c], 1, s->max[c]));
+    }
+    set_meta(metadata, "lavfi.psnr.mse_avg", 0, mse);
+    set_meta(metadata, "lavfi.psnr.psnr_avg", 0, get_psnr(mse, 1, s->average_max));
+
+    /* scene change detection score */
+    s->scene_score = get_scene_score(ctx, ref);
+    if (s->scene_score >= s->scd_thresh && s->nb_shd >= 48) {
+        av_log(s, AV_LOG_WARNING, "ImgHashScene: n:%"PRId64"-%"PRId64" hd_avg:%0.3lf hd_min:%0.3lf hd_max:%0.3lf scd:%0.2lf\n",
+               (s->nb_frames - s->nb_shd), s->nb_frames - 1, (s->shd / s->nb_shd), s->smin_hd, s->smax_hd, s->scene_score);
+        s->shd = 0;
+        s->nb_shd = 0;
+        s->smin_hd = 0;
+        s->smax_hd = 0;
+    }
+
+    /* limit the highest value so we cut off at perceptual difference match */
+    if (s->hash_type_i == PHASH || s->hash_type_i == AVERAGE)
+        hd_limit = 5;
+    else if (s->hash_type_i == MARRHILDRETH)
+        hd_limit = 30;
+    else if (s->hash_type_i == RADIALVARIANCE)
+        hd_limit = 0.9;
+    else if (s->hash_type_i == BLOCKMEAN1)
+        hd_limit = 12;
+    else if (s->hash_type_i == BLOCKMEAN2)
+        hd_limit = 48;
+    else if (s->hash_type_i == COLORMOMENT)
+        hd_limit = 8;
+
+    /* get ref / enc perceptual hashes and calc hamming distance difference value */
+    hd = getScore(ref, master, ref->format, s->hash_type_i);
+    s->hd += FFMIN(hd, hd_limit);
+
+    /* scene hamming distance avg */
+    s->shd += FFMIN(hd, hd_limit);
+    s->nb_shd++;
+    av_log(s, AV_LOG_DEBUG, "ImgHashFrame: hd:%0.3lf scd:%0.2lf\n", hd, s->scene_score);
+
+    s->min_hd = FFMIN(s->min_hd, hd);
+    s->max_hd = FFMAX(s->max_hd, hd);
+    s->smin_hd = FFMIN(s->smin_hd, hd);
+    s->smax_hd = FFMAX(s->smax_hd, hd);
+
+    if (s->stats_file) {
+        if (s->stats_version == 2 && !s->stats_header_written) {
+            fprintf(s->stats_file, "psnr_log_version:2 fields:n");
+            fprintf(s->stats_file, ",mse_avg");
+            for (j = 0; j < s->nb_components; j++) {
+                fprintf(s->stats_file, ",mse_%c", s->comps[j]);
+            }
+            fprintf(s->stats_file, ",psnr_avg");
+            for (j = 0; j < s->nb_components; j++) {
+                fprintf(s->stats_file, ",psnr_%c", s->comps[j]);
+            }
+            if (s->stats_add_max) {
+                fprintf(s->stats_file, ",max_avg");
+                for (j = 0; j < s->nb_components; j++) {
+                    fprintf(s->stats_file, ",max_%c", s->comps[j]);
+                }
+            }
+            fprintf(s->stats_file, "\n");
+            s->stats_header_written = 1;
+        }
+        fprintf(s->stats_file,
+                "n:%"PRId64" phqm_avg:%0.3f phqm_min:%0.3f phqm_max:%0.3f scd:%0.2f mse_avg:%0.2f ",
+                s->nb_frames, hd, s->min_hd, s->max_hd, s->scene_score, mse);
+        for (j = 0; j < s->nb_components; j++) {
+            c = s->is_rgb ? s->rgba_map[j] : j;
+            fprintf(s->stats_file, "mse_%c:%0.2f ", s->comps[j], comp_mse[c]);
+        }
+        fprintf(s->stats_file, "psnr_avg:%0.2f ", get_psnr(mse, 1, s->average_max));
+        for (j = 0; j < s->nb_components; j++) {
+            c = s->is_rgb ? s->rgba_map[j] : j;
+            fprintf(s->stats_file, "psnr_%c:%0.2f ", s->comps[j],
+                    get_psnr(comp_mse[c], 1, s->max[c]));
+        }
+        if (s->stats_version == 2 && s->stats_add_max) {
+            fprintf(s->stats_file, "max_avg:%d ", s->average_max);
+            for (j = 0; j < s->nb_components; j++) {
+                c = s->is_rgb ? s->rgba_map[j] : j;
+                fprintf(s->stats_file, "max_%c:%d ", s->comps[j], s->max[c]);
+            }
+        }
+        fprintf(s->stats_file, "\n");
+    }
+
+    return ff_filter_frame(ctx->outputs[0], master);
+}
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    ImgHashContext *s = ctx->priv;
+
+    s->min_mse = +INFINITY;
+    s->max_mse = -INFINITY;
+
+    if (s->stats_file_str) {
+        if (s->stats_version < 2 && s->stats_add_max) {
+            av_log(ctx, AV_LOG_ERROR,
+                "stats_add_max was specified but stats_version < 2.\n" );
+            return AVERROR(EINVAL);
+        }
+        if (!strcmp(s->stats_file_str, "-")) {
+            s->stats_file = stdout;
+        } else {
+            s->stats_file = fopen(s->stats_file_str, "w");
+            if (!s->stats_file) {
+                int err = AVERROR(errno);
+                char buf[128];
+                av_strerror(err, buf, sizeof(buf));
+                av_log(ctx, AV_LOG_ERROR, "Could not open stats file %s: %s\n",
+                       s->stats_file_str, buf);
+                return err;
+            }
+        }
+    }
+
+    if (s->hash_type) {
+        if (!strcmp(s->hash_type, "phash"))
+            s->hash_type_i = PHASH;
+        else if (!strcmp(s->hash_type, "colormoment")) {
+            s->hash_type_i = COLORMOMENT;
+        } else if (!strcmp(s->hash_type, "average"))
+            s->hash_type_i = AVERAGE;
+        else if (!strcmp(s->hash_type, "marrhildreth"))
+            s->hash_type_i = MARRHILDRETH;
+        else if (!strcmp(s->hash_type, "radialvariance"))
+            s->hash_type_i = RADIALVARIANCE;
+        else if (!strcmp(s->hash_type, "blockmean1"))
+            s->hash_type_i = BLOCKMEAN1;
+        else if (!strcmp(s->hash_type, "blockmean2"))
+            s->hash_type_i = BLOCKMEAN2;
+        else {
+            av_log(s, AV_LOG_ERROR, "Bad hash_type given %s\n", s->hash_type);
+            return AVERROR(EINVAL);
+        }
+    }
+
+    s->sad = ff_scene_sad_get_fn(8);
+    if (!s->sad)
+        return AVERROR(EINVAL);
+
+    s->fs.on_event = do_phqm;
+    return 0;
+}
+
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_GRAY8, AV_PIX_FMT_GRAY9, AV_PIX_FMT_GRAY10, AV_PIX_FMT_GRAY12, AV_PIX_FMT_GRAY14, AV_PIX_FMT_GRAY16,
+#define PF_NOALPHA(suf) AV_PIX_FMT_YUV420##suf,  AV_PIX_FMT_YUV422##suf,  AV_PIX_FMT_YUV444##suf
+#define PF_ALPHA(suf)   AV_PIX_FMT_YUVA420##suf, AV_PIX_FMT_YUVA422##suf, AV_PIX_FMT_YUVA444##suf
+#define PF(suf)         PF_NOALPHA(suf), PF_ALPHA(suf)
+        PF(P), PF(P9), PF(P10), PF_NOALPHA(P12), PF_NOALPHA(P14), PF(P16),
+        AV_PIX_FMT_YUV440P, AV_PIX_FMT_YUV411P, AV_PIX_FMT_YUV410P,
+        AV_PIX_FMT_YUVJ411P, AV_PIX_FMT_YUVJ420P, AV_PIX_FMT_YUVJ422P,
+        AV_PIX_FMT_YUVJ440P, AV_PIX_FMT_YUVJ444P,
+        AV_PIX_FMT_GBRP, AV_PIX_FMT_GBRP9, AV_PIX_FMT_GBRP10,
+        AV_PIX_FMT_GBRP12, AV_PIX_FMT_GBRP14, AV_PIX_FMT_GBRP16,
+        AV_PIX_FMT_GBRAP, AV_PIX_FMT_GBRAP10, AV_PIX_FMT_GBRAP12, AV_PIX_FMT_GBRAP16,
+        AV_PIX_FMT_NONE
+    };
+
+    AVFilterFormats *fmts_list = ff_make_format_list(pix_fmts);
+    if (!fmts_list)
+        return AVERROR(ENOMEM);
+    return ff_set_common_formats(ctx, fmts_list);
+}
+
+static int config_input_ref(AVFilterLink *inlink)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(inlink->format);
+    AVFilterContext *ctx  = inlink->dst;
+    ImgHashContext *s = ctx->priv;
+    double average_max;
+    unsigned sum;
+    int j;
+
+    s->nb_components = desc->nb_components;
+    if (ctx->inputs[0]->w != ctx->inputs[1]->w ||
+        ctx->inputs[0]->h != ctx->inputs[1]->h) {
+        av_log(ctx, AV_LOG_ERROR, "Width and height of input videos must be same.\n");
+        return AVERROR(EINVAL);
+    }
+    if (ctx->inputs[0]->format != ctx->inputs[1]->format) {
+        av_log(ctx, AV_LOG_ERROR, "Inputs must be of same pixel format.\n");
+        return AVERROR(EINVAL);
+    }
+
+    s->max[0] = (1 << desc->comp[0].depth) - 1;
+    s->max[1] = (1 << desc->comp[1].depth) - 1;
+    s->max[2] = (1 << desc->comp[2].depth) - 1;
+    s->max[3] = (1 << desc->comp[3].depth) - 1;
+
+    s->is_rgb = ff_fill_rgba_map(s->rgba_map, inlink->format) >= 0;
+    s->comps[0] = s->is_rgb ? 'r' : 'y' ;
+    s->comps[1] = s->is_rgb ? 'g' : 'u' ;
+    s->comps[2] = s->is_rgb ? 'b' : 'v' ;
+    s->comps[3] = 'a';
+
+    s->planeheight[1] = s->planeheight[2] = AV_CEIL_RSHIFT(inlink->h, desc->log2_chroma_h);
+    s->planeheight[0] = s->planeheight[3] = inlink->h;
+    s->planewidth[1]  = s->planewidth[2]  = AV_CEIL_RSHIFT(inlink->w, desc->log2_chroma_w);
+    s->planewidth[0]  = s->planewidth[3]  = inlink->w;
+    sum = 0;
+    for (j = 0; j < s->nb_components; j++)
+        sum += s->planeheight[j] * s->planewidth[j];
+    average_max = 0;
+    for (j = 0; j < s->nb_components; j++) {
+        s->planeweight[j] = (double) s->planeheight[j] * s->planewidth[j] / sum;
+        average_max += s->max[j] * s->planeweight[j];
+    }
+    s->average_max = lrint(average_max);
+
+    s->dsp.sse_line = desc->comp[0].depth > 8 ? sse_line_16bit : sse_line_8bit;
+    if (ARCH_X86)
+        ff_psnr_init_x86(&s->dsp, desc->comp[0].depth);
+
+    return 0;
+}
+
+static int config_output(AVFilterLink *outlink)
+{
+    AVFilterContext *ctx = outlink->src;
+    ImgHashContext *s = ctx->priv;
+    AVFilterLink *mainlink = ctx->inputs[0];
+    int ret;
+
+    ret = ff_framesync_init_dualinput(&s->fs, ctx);
+    if (ret < 0)
+        return ret;
+    outlink->w = mainlink->w;
+    outlink->h = mainlink->h;
+    outlink->time_base = mainlink->time_base;
+    outlink->sample_aspect_ratio = mainlink->sample_aspect_ratio;
+    outlink->frame_rate = mainlink->frame_rate;
+    if ((ret = ff_framesync_configure(&s->fs)) < 0)
+        return ret;
+
+    return 0;
+}
+
+static int activate(AVFilterContext *ctx)
+{
+    ImgHashContext *s = ctx->priv;
+    return ff_framesync_activate(&s->fs);
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    ImgHashContext *s = ctx->priv;
+
+    if (s->nb_frames > 0) {
+        int j;
+        char buf[256];
+
+        buf[0] = 0;
+        for (j = 0; j < s->nb_components; j++) {
+            int c = s->is_rgb ? s->rgba_map[j] : j;
+            av_strlcatf(buf, sizeof(buf), " %c:%f", s->comps[j],
+                        get_psnr(s->mse_comp[c], s->nb_frames, s->max[c]));
+        }
+        av_log(ctx, AV_LOG_WARNING, "PHQM average:%f PSNR%s average:%f min:%f max:%f\n",
+               s->hd / s->nb_frames,
+               buf,
+               get_psnr(s->mse, s->nb_frames, s->average_max),
+               get_psnr(s->max_mse, 1, s->average_max),
+               get_psnr(s->min_mse, 1, s->average_max));
+    }
+
+    ff_framesync_uninit(&s->fs);
+
+    if (s->stats_file && s->stats_file != stdout)
+        fclose(s->stats_file);
+    av_frame_free(&s->prev_picref);
+}
+
+static const AVFilterPad img_hash_inputs[] = {
+    {
+        .name         = "main",
+        .type         = AVMEDIA_TYPE_VIDEO,
+    },{
+        .name         = "reference",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .config_props = config_input_ref,
+    },
+    { NULL }
+};
+
+static const AVFilterPad img_hash_outputs[] = {
+    {
+        .name          = "default",
+        .type          = AVMEDIA_TYPE_VIDEO,
+        .config_props  = config_output,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_img_hash= {
+    .name          = "img_hash",
+    .description   = NULL_IF_CONFIG_SMALL("PHQM: Calculate the Perceptual Hash Hamming Difference between two video streams."),
+    .preinit       = img_hash_framesync_preinit,
+    .init          = init,
+    .uninit        = uninit,
+    .query_formats = query_formats,
+    .activate      = activate,
+    .priv_size     = sizeof(ImgHashContext),
+    .priv_class    = &img_hash_class,
+    .inputs        = img_hash_inputs,
+    .outputs       = img_hash_outputs,
+};
diff --git a/libavfilter/vf_perceptual.c b/libavfilter/vf_perceptual.c
new file mode 100644
index 0000000..be67a48
--- /dev/null
+++ b/libavfilter/vf_perceptual.c
@@ -0,0 +1,160 @@
+/*
+ * PERCEPTUAL FILTER: Chris Kennedy (C) 2019
+ *
+ * Perceptual scoring frame matching
+ *
+ * Tags decoded frames with a perceptual score for the x264
+ * encoder to use.
+ *
+ * License: Apache 2.0
+ *
+ */
+
+/**
+ * @file
+ * perceptual: Perceptual Scoring
+ */
+
+#include "libavutil/adler32.h"
+#include "libavutil/avstring.h"
+#include "libavutil/common.h"
+#include "libavutil/file.h"
+#include "libavutil/opt.h"
+#include "libavutil/imgutils.h"
+#include "libavutil/pixdesc.h"
+#include "libavutil/timestamp.h"
+#include "libavfilter/lswsutils.h"
+#include "avfilter.h"
+#include "formats.h"
+#include "internal.h"
+#include "video.h"
+#include "img_hash.h"
+
+typedef struct {
+    const AVClass *class;
+    char *hash_type;
+    double score_multiplier;
+    double score_factor;
+    int hash_type_i;
+    uint64_t frame_count; // frame counter/number
+    AVFrame *lastframe;
+
+    void *priv; // private structure for filter, std FFmpeg API
+} PerceptualContext;
+
+// decides color space raw frame format used via standard LibAV API
+static int query_formats(AVFilterContext *ctx)
+{
+    static const enum AVPixelFormat pix_fmts[] = {
+        AV_PIX_FMT_YUV420P, AV_PIX_FMT_BGR24, AV_PIX_FMT_GRAY8, AV_PIX_FMT_NONE
+    };
+    return ff_set_common_formats(ctx, ff_make_format_list(pix_fmts));
+}
+
+static av_cold int init(AVFilterContext *ctx)
+{
+    PerceptualContext *perceptual = ctx->priv;
+
+    if (!(perceptual->priv = av_mallocz(sizeof(PerceptualContext))))
+        return AVERROR(ENOMEM);
+
+    if (perceptual->hash_type) {
+        if (!strcmp(perceptual->hash_type, "phash"))
+            perceptual->hash_type_i = PHASH;
+        else if (!strcmp(perceptual->hash_type, "colormoment")) {
+            perceptual->hash_type_i = COLORMOMENT;
+            perceptual->score_multiplier = 1.0;
+        } else if (!strcmp(perceptual->hash_type, "average"))
+            perceptual->hash_type_i = AVERAGE;
+        else if (!strcmp(perceptual->hash_type, "marrhildreth"))
+            perceptual->hash_type_i = MARRHILDRETH;
+        else if (!strcmp(perceptual->hash_type, "radialvariance"))
+            perceptual->hash_type_i = RADIALVARIANCE;
+        else if (!strcmp(perceptual->hash_type, "blockmean1"))
+            perceptual->hash_type_i = BLOCKMEAN1;
+        else if (!strcmp(perceptual->hash_type, "blockmean2"))
+            perceptual->hash_type_i = BLOCKMEAN2;
+        else {
+            av_log(perceptual, AV_LOG_ERROR, "Bad hash_type given %s\n", perceptual->hash_type);
+            return AVERROR(ENOMEM);
+        }
+    }
+    return 0;
+}
+
+static av_cold void uninit(AVFilterContext *ctx)
+{
+    PerceptualContext *perceptual = ctx->priv;
+
+    av_free(perceptual->priv);
+}
+
+static int filter_frame(AVFilterLink *inlink, AVFrame *in)
+{
+    AVFilterContext *ctx = inlink->dst;
+    AVFilterLink *outlink= inlink->dst->outputs[0];
+    PerceptualContext *perceptual = ctx->priv;
+    double score = 0;
+
+    perceptual->frame_count++;
+
+    /* compare last frame with this current frame */
+    if (perceptual->frame_count > 1) {
+        score = getScore(perceptual->lastframe, in, inlink->format, perceptual->hash_type_i);
+        av_log(perceptual, AV_LOG_DEBUG, "Perceptual: %s hamming score %0.1lf\n", perceptual->hash_type, score);
+
+        /* mark frame, normalize to percentage */
+        in->perceptual_score = .01 * FFMIN((score * perceptual->score_multiplier), 100);
+        in->perceptual_score_factor = perceptual->score_factor;
+
+        av_frame_free(&perceptual->lastframe);
+    }
+    /* save last frame */
+    perceptual->lastframe = av_frame_clone(in);
+
+    av_log(perceptual, AV_LOG_DEBUG, "Perceptual: %%%0.1lf frame change\n", in->perceptual_score * 100);
+
+    return ff_filter_frame(outlink, in);
+}
+
+#define OFFSET(x) offsetof(PerceptualContext, x)
+#define FLAGS AV_OPT_FLAG_VIDEO_PARAM | AV_OPT_FLAG_FILTERING_PARAM
+static const AVOption perceptual_options[] = {
+    { "hash_type", "options: phash, colormoment, average", OFFSET(hash_type), AV_OPT_TYPE_STRING, {.str = "phash"}, .flags = FLAGS },
+    { "score_multiplier", "multiply the hamming score result by this value. 2.0 by default", OFFSET(score_multiplier),
+        AV_OPT_TYPE_DOUBLE, {.dbl = 2.0}, 0, 100, .flags = FLAGS },
+    { "score_factor", "factor to decrease compression, multiplier for bitrate, range for crf. 2.0 default", OFFSET(score_factor),
+        AV_OPT_TYPE_DOUBLE, {.dbl = 2.0}, 0, 1000, .flags = FLAGS },
+    { NULL }
+};
+
+AVFILTER_DEFINE_CLASS(perceptual);
+
+static const AVFilterPad avfilter_vf_perceptual_inputs[] = {
+    {
+        .name         = "default",
+        .type         = AVMEDIA_TYPE_VIDEO,
+        .filter_frame = filter_frame,
+    },
+    { NULL }
+};
+
+static const AVFilterPad avfilter_vf_perceptual_outputs[] = {
+    {
+        .name = "default",
+        .type = AVMEDIA_TYPE_VIDEO,
+    },
+    { NULL }
+};
+
+AVFilter ff_vf_perceptual = {
+    .name          = "perceptual",
+    .description   = NULL_IF_CONFIG_SMALL("Perceptual Scoring Filter"),
+    .priv_size     = sizeof(PerceptualContext),
+    .priv_class    = &perceptual_class,
+    .query_formats = query_formats,
+    .init          = init,
+    .uninit        = uninit,
+    .inputs        = avfilter_vf_perceptual_inputs,
+    .outputs       = avfilter_vf_perceptual_outputs,
+};
diff --git a/libavutil/frame.c b/libavutil/frame.c
index dcf1fc3..94e7644 100644
--- a/libavutil/frame.c
+++ b/libavutil/frame.c
@@ -163,6 +163,8 @@ FF_ENABLE_DEPRECATION_WARNINGS
     frame->color_range         = AVCOL_RANGE_UNSPECIFIED;
     frame->chroma_location     = AVCHROMA_LOC_UNSPECIFIED;
     frame->flags               = 0;
+    frame->perceptual_score    = -1;
+    frame->perceptual_score_factor    = 2.0;
 }
 
 static void free_side_data(AVFrameSideData **ptr_sd)
@@ -373,6 +375,8 @@ FF_ENABLE_DEPRECATION_WARNINGS
     dst->colorspace             = src->colorspace;
     dst->color_range            = src->color_range;
     dst->chroma_location        = src->chroma_location;
+    dst->perceptual_score       = src->perceptual_score;
+    dst->perceptual_score_factor       = src->perceptual_score_factor;
 
     av_dict_copy(&dst->metadata, src->metadata, 0);
 
@@ -453,6 +457,8 @@ int av_frame_ref(AVFrame *dst, const AVFrame *src)
     dst->channels       = src->channels;
     dst->channel_layout = src->channel_layout;
     dst->nb_samples     = src->nb_samples;
+    dst->perceptual_score = src->perceptual_score;
+    dst->perceptual_score_factor = src->perceptual_score_factor;
 
     ret = frame_copy_props(dst, src, 0);
     if (ret < 0)
diff --git a/libavutil/frame.h b/libavutil/frame.h
index 5d3231e..c9df011 100644
--- a/libavutil/frame.h
+++ b/libavutil/frame.h
@@ -672,6 +672,13 @@ typedef struct AVFrame {
      * for the target frame's private_ref field.
      */
     AVBufferRef *private_ref;
+
+    /**
+     * perceptual score
+     * 0.00 - 1.00 percentage of perceptual match to the previous frame
+     */
+    float perceptual_score;
+    float perceptual_score_factor;
 } AVFrame;
 
 #if FF_API_FRAME_GET_SET
